# сделано руками из копирования 2.2.10
# link on documentation: https://sphinxsearch.com/docs/sphinx3.html
source:
    csvpipe_delimiter:
        link: 'http://sphinxsearch.com/docs/current.html#conf-csvpipe-delimiter'
        multi_value: false
        description: "csvpipe source fields delimiter. Optional, default value is ','.\nIntroduced in version 2.2.1-beta.\nExample:\ncsvpipe_delimiter = ;"
    unpack_mysqlcompress_maxsize:
        link: 'http://sphinxsearch.com/docs/current.html#conf-unpack-mysqlcompress-maxsize'
        multi_value: false
        description: "Buffer size for UNCOMPRESS()ed data.\nOptional, default value is 16M.\nIntroduced in version 0.9.9-rc1.\n\nWhen using unpack_mysqlcompress,\ndue to implementation intricacies it is not possible to deduce the required buffer size\nfrom the compressed data. So the buffer must be preallocated in advance, and unpacked\ndata can not go over the buffer size. This option lets you control the buffer size,\nboth to limit indexer memory use, and to enable unpacking\nof really long data fields if necessary.\nExample:\nunpack_mysqlcompress_maxsize = 1M"
    unpack_mysqlcompress:
        link: 'http://sphinxsearch.com/docs/current.html#conf-unpack-mysqlcompress'
        multi_value: true
        description: "Columns to unpack using MySQL UNCOMPRESS() algorithm.\nMulti-value, optional, default value is empty list of columns.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nIntroduced in version 0.9.9-rc1.\n\nColumns specified using this directive will be unpacked by indexer\nusing modified zlib algorithm used by MySQL COMPRESS() and UNCOMPRESS() functions.\nWhen indexing on a different box than the database, this lets you offload the database, and save on network traffic.\nThe feature is only available if zlib and zlib-devel were both available during build time.\nExample:\nunpack_mysqlcompress = body_compressed\nunpack_mysqlcompress = description_compressed"
    unpack_zlib:
        link: 'http://sphinxsearch.com/docs/current.html#conf-unpack-zlib'
        multi_value: true
        description: "Columns to unpack using zlib (aka deflate, aka gunzip).\nMulti-value, optional, default value is empty list of columns.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nIntroduced in version 0.9.9-rc1.\n\nColumns specified using this directive will be unpacked by indexer\nusing standard zlib algorithm (called deflate and also implemented by gunzip).\nWhen indexing on a different box than the database, this lets you offload the database, and save on network traffic.\nThe feature is only available if zlib and zlib-devel were both available during build time.\nExample:\nunpack_zlib = col1\nunpack_zlib = col2"
    mssql_winauth:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mssql-winauth'
        multi_value: false
        description: "MS SQL Windows authentication flag.\nBoolean, optional, default value is 0 (false).\nApplies to mssql source type only.\nIntroduced in version 0.9.9-rc1.\n\nWhether to use currently logged in Windows account credentials for\nauthentication when connecting to MS SQL Server. Note that when running\nsearchd as a service, account user can differ\nfrom the account you used to install the service.\nExample:\nmssql_winauth = 1"
    xmlpipe_fixup_utf8:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-fixup-utf8'
        multi_value: false
        description: "Perform Sphinx-side UTF-8 validation and filtering to prevent XML parser from choking on non-UTF-8 documents.\nOptional, default is 0.\nApplies to xmlpipe2 source type only.\n\nUnder certain occasions it might be hard or even impossible to guarantee\nthat the incoming XMLpipe2 document bodies are in perfectly valid and\nconforming UTF-8 encoding.  For instance, documents with national\nsingle-byte encodings could sneak into the stream. libexpat XML parser\nis fragile, meaning that it will stop processing in such cases.\nUTF8 fixup feature lets you avoid that. When fixup is enabled,\nSphinx will preprocess the incoming stream before passing it to the\nXML parser and replace invalid UTF-8 sequences with spaces.\nExample:\nxmlpipe_fixup_utf8 = 1"
    xmlpipe_attr_json:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-json'
        multi_value: true
        description: "JSON attribute declaration.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nIntroduced in version 2.1.1-beta.\n\nThis directive is used to declare that the contents of a given\nXML tag are to be treated as a JSON document and stored into a Sphinx\nindex for later use. Refer to Section\_12.1.24, “sql_attr_json”\nfor more details on the JSON attributes.\nExample:\nxmlpipe_attr_json = properties"
    xmlpipe_attr_string:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-string'
        multi_value: true
        description: "xmlpipe string declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\nIntroduced in version 1.10-beta.\n\nThis setting declares a string attribute tag in xmlpipe2 stream.\nThe contents of the specified tag will be parsed and stored as a string value.\nExample:\nxmlpipe_attr_string = subject"
    xmlpipe_attr_multi_64:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-multi-64'
        multi_value: true
        description: "xmlpipe MVA attribute declaration. Declares the BIGINT (signed 64-bit integer) MVA attribute.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\n\nThis setting declares an MVA attribute tag in xmlpipe2 stream.\nThe contents of the specified tag will be parsed and a list of integers\nthat will constitute the MVA will be extracted, similar to how\nsql_attr_multi parses\nSQL column contents when 'field' MVA source type is specified.\nExample:\nxmlpipe_attr_multi_64 = taglist"
    xmlpipe_attr_multi:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-multi'
        multi_value: true
        description: "xmlpipe MVA attribute declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\n\nThis setting declares an MVA attribute tag in xmlpipe2 stream.\nThe contents of the specified tag will be parsed and a list of integers\nthat will constitute the MVA will be extracted, similar to how\nsql_attr_multi parses\nSQL column contents when 'field' MVA source type is specified.\nExample:\nxmlpipe_attr_multi = taglist"
    xmlpipe_attr_float:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-float'
        multi_value: true
        description: "xmlpipe floating point attribute declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\nSyntax fully matches that of sql_attr_float.\nExample:\nxmlpipe_attr_float = lat_radians\nxmlpipe_attr_float = long_radians"
    xmlpipe_attr_timestamp:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-timestamp'
        multi_value: true
        description: "xmlpipe UNIX timestamp attribute declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\nSyntax fully matches that of sql_attr_timestamp.\nExample:\nxmlpipe_attr_timestamp = published"
    xmlpipe_attr_bool:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-bool'
        multi_value: true
        description: "xmlpipe boolean attribute declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\nSyntax fully matches that of sql_attr_bool.\nExample:\nxmlpipe_attr_bool = is_deleted # will be packed to 1 bit"
    xmlpipe_attr_bigint:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-bigint'
        multi_value: true
        description: "xmlpipe signed 64-bit integer attribute declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\nSyntax fully matches that of sql_attr_bigint.\nExample:\nxmlpipe_attr_bigint = my_bigint_id"
    xmlpipe_attr_uint:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-attr-uint'
        multi_value: true
        description: "xmlpipe integer attribute declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only.\nSyntax fully matches that of sql_attr_uint.\nExample:\nxmlpipe_attr_uint = author_id"
    xmlpipe_field_string:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-field-string'
        multi_value: true
        description: "xmlpipe field and string attribute declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only. Refer to Section\_3.9, “xmlpipe2 data source”.\nIntroduced in version 1.10-beta.\n\nMakes the specified XML element indexed as both a full-text field and a string attribute.\nEquivalent to <sphinx:field name=\"field\" attr=\"string\"/> declaration within the XML file.\nExample:\nxmlpipe_field_string = subject"
    xmlpipe_field:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-field'
        multi_value: true
        description: "xmlpipe field declaration.\nMulti-value, optional.\nApplies to xmlpipe2 source type only. Refer to Section\_3.9, “xmlpipe2 data source”.\nExample:\nxmlpipe_field = subject\nxmlpipe_field = content"
    xmlpipe_command:
        link: 'http://sphinxsearch.com/docs/current.html#conf-xmlpipe-command'
        multi_value: false
        description: "Shell command that invokes xmlpipe2 stream producer.\nMandatory.\nApplies to xmlpipe2 source types only.\n\nSpecifies a command that will be executed and which output\nwill be parsed for documents. Refer to Section\_3.9, “xmlpipe2 data source” for specific format description.\nExample:\nxmlpipe_command = cat /home/sphinx/test.xml"
    sql_ranged_throttle:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-ranged-throttle'
        multi_value: false
        description: "Ranged query throttling period, in milliseconds.\nOptional, default is 0 (no throttling).\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nThrottling can be useful when indexer imposes too much load on the\ndatabase server. It causes the indexer to sleep for given amount of\nmilliseconds once per each ranged query step. This sleep is unconditional,\nand is performed before the fetch query.\nExample:\nsql_ranged_throttle = 1000 # sleep for 1 sec before each query step"
    sql_query_post_index:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-query-post-index'
        multi_value: false
        description: "Post-index query.\nOptional, default value is empty.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nThis query is executed when indexing is fully and successfully completed.\nIf this query produces errors, they are reported as warnings,\nbut indexing is not terminated. It's result set is ignored.\n$maxid macro can be used in its text; it will be\nexpanded to maximum document ID which was actually fetched\nfrom the database during indexing. If no documents were indexed,\n$maxid will be expanded to 0.\nExample:\nsql_query_post_index = REPLACE INTO counters ( id, val ) \\\n    VALUES ( 'max_indexed_id', $maxid )"
    sql_query_post:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-query-post'
        multi_value: false
        description: "Post-fetch query.\nOptional, default value is empty.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nThis query is executed immediately after sql_query\ncompletes successfully. When post-fetch query produces errors,\nthey are reported as warnings, but indexing is not terminated.\nIt's result set is ignored. Note that indexing is not yet completed\nat the point when this query gets executed, and further indexing still may fail.\nTherefore, any permanent updates should not be done from here.\nFor instance, updates on helper table that permanently change\nthe last successfully indexed ID should not be run from post-fetch\nquery; they should be run from post-index query instead.\nExample:\nsql_query_post = DROP TABLE my_tmp_table"
    sql_file_field:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-file-field'
        multi_value: false
        description: "File based field declaration.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nIntroduced in version 1.10-beta.\n\nThis directive makes indexer interpret field contents\nas a file name, and load and index the referred file.  Files larger than\nmax_file_field_buffer\nin size are skipped.  Any errors during the file loading (IO errors, missed\nlimits, etc) will be reported as indexing warnings and will not early\nterminate the indexing.  No content will be indexed for such files.\nExample:\nsql_file_field = my_file_path # load and index files referred to by my_file_path"
    sql_field_string:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-field-string'
        multi_value: true
        description: "Combined string attribute and full-text field declaration.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nIntroduced in version 1.10-beta.\n\nsql_attr_string only stores the column\nvalue but does not full-text index it.  In some cases it might be desired to both full-text\nindex the column and store it as attribute.  sql_field_string lets you do\nexactly that. Both the field and the attribute will be named the same.\nExample:\nsql_field_string = title # will be both indexed and stored"
    sql_column_buffers:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-column-buffers'
        multi_value: false
        description: "Per-column buffer sizes.\nOptional, default is empty (deduce the sizes automatically).\nApplies to odbc, mssql source types only.\nIntroduced in version 2.0.1-beta.\n\nODBC and MS SQL drivers sometimes can not return the maximum\nactual column size to be expected. For instance, NVARCHAR(MAX) columns\nalways report their length as 2147483647 bytes to\nindexer even though the actually used length\nis likely considerably less. However, the receiving buffers still\nneed to be allocated upfront, and their sizes have to be determined.\nWhen the driver does not report the column length at all, Sphinx\nallocates default 1 KB buffers for each non-char column, and 1 MB\nbuffers for each char column. Driver-reported column length\nalso gets clamped by an upper limit of 8 MB, so in case the\ndriver reports (almost) a 2 GB column length, it will be clamped\nand a 8 MB buffer will be allocated instead for that column.\nThese hard-coded limits can be overridden using the\nsql_column_buffers directive, either in order\nto save memory on actually shorter columns, or overcome\nthe 8 MB limit on actually longer columns. The directive values\nmust be a comma-separated lists of selected column names and sizes:\n\nsql_column_buffers = <colname>=<size>[K|M] [, ...]\n\nExample:\nsql_query = SELECT id, mytitle, mycontent FROM documents\nsql_column_buffers = mytitle=64K, mycontent=10M"
    sql_attr_json:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-json'
        multi_value: true
        description: "JSON attribute declaration.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nIntroduced in version 2.1.1-beta.\n\nWhen indexing JSON attributes, Sphinx expects a text field\nwith JSON formatted data. As of 2.2.1-beta JSON attributes supports arbitrary\nJSON data with no limitation in nested levels or types.\n\n{\n    \"id\": 1,\n    \"gid\": 2,\n    \"title\": \"some title\",\n    \"tags\": [\n        \"tag1\",\n        \"tag2\",\n        \"tag3\"\n\t\t{\n\t\t\t\"one\": \"two\",\n\t\t\t\"three\": [4, 5]\n\t\t}\n    ]\n}\n\nThese attributes allow Sphinx to work with documents without a fixed set of\nattribute columns. When you filter on a key of a JSON attribute, documents\nthat don't include the key will simply be ignored.\n\nYou can read more on JSON attributes in \nhttp://sphinxsearch.com/blog/2013/08/08/full-json-support-in-trunk/.\nExample:\nsql_attr_json = properties"
    sql_attr_string:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-string'
        multi_value: true
        description: "String attribute declaration.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nIntroduced in version 1.10-beta.\n\nString attributes can store arbitrary strings attached to every document.\nThere's a fixed size limit of 4 MB per value. Also, searchd\nwill currently cache all the values in RAM, which is an additional implicit limit.\n\nStarting from 2.0.1-beta string attributes can be used for sorting and\ngrouping(ORDER BY, GROUP BY, WITHIN GROUP ORDER BY). Note that attributes\ndeclared using sql_attr_string will not be full-text\nindexed; you can use sql_field_string\ndirective for that.\nExample:\nsql_attr_string = title # will be stored but will not be indexed"
    sql_attr_multi:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-multi'
        multi_value: true
        description: "Multi-valued attribute (MVA) declaration.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nPlain attributes only allow to attach 1 value per each document.\nHowever, there are cases (such as tags or categories) when it is\ndesired to attach multiple values of the same attribute and be able\nto apply filtering or grouping to value lists.\n\nThe declaration format is as follows (backslashes are for clarity only;\neverything can be declared in a single line as well):\n\nsql_attr_multi = ATTR-TYPE ATTR-NAME 'from' SOURCE-TYPE \\\n    [;QUERY] \\\n    [;RANGE-QUERY]\n\nwhere\nATTR-TYPE is 'uint', 'bigint' or 'timestamp'\nSOURCE-TYPE is 'field', 'query', or 'ranged-query'\nQUERY is SQL query used to fetch all ( docid, attrvalue ) pairs\nRANGE-QUERY is SQL query used to fetch min and max ID values, similar to 'sql_query_range'\n\n\nExample:\nsql_attr_multi = uint tag from query; SELECT id, tag FROM tags\nsql_attr_multi = bigint tag from ranged-query; \\\n    SELECT id, tag FROM tags WHERE id>=$start AND id<=$end; \\\n    SELECT MIN(id), MAX(id) FROM tags"
    sql_attr_float:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-float'
        multi_value: true
        description: "Floating point attribute declaration.\nMulti-value (there might be multiple attributes declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nThe values will be stored in single precision, 32-bit IEEE 754 format.\nRepresented range is approximately from 1e-38 to 1e+38. The amount\nof decimal digits that can be stored precisely is approximately 7.\nOne important usage of the float attributes is storing latitude\nand longitude values (in radians), for further usage in query-time\ngeosphere distance calculations.\nExample:\nsql_attr_float = lat_radians\nsql_attr_float = long_radians"
    sql_attr_timestamp:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-timestamp'
        multi_value: true
        description: "UNIX timestamp attribute declaration.\nMulti-value (there might be multiple attributes declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nTimestamps can store date and time in the range of Jan 01, 1970\nto Jan 19, 2038 with a precision of one second.\nThe expected column value should be a timestamp in UNIX format, ie. 32-bit unsigned\ninteger number of seconds elapsed since midnight, January 01, 1970, GMT.\nTimestamps are internally stored and handled as integers everywhere.\nBut in addition to working with timestamps as integers, it's also legal\nto use them along with different date-based functions, such as time segments\nsorting mode, or day/week/month/year extraction for GROUP BY.\n\nNote that DATE or DATETIME column types in MySQL can not be directly\nused as timestamp attributes in Sphinx; you need to explicitly convert such\ncolumns using UNIX_TIMESTAMP function (if data is in range).\n\nNote timestamps can not represent dates before January 01, 1970,\nand UNIX_TIMESTAMP() in MySQL will not return anything expected.\nIf you only needs to work with dates, not times, consider TO_DAYS()\nfunction in MySQL instead.\nExample:\n# sql_query = ... UNIX_TIMESTAMP(added_datetime) AS added_ts ...\nsql_attr_timestamp = added_ts"
    sql_attr_bigint:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-bigint'
        multi_value: true
        description: "64-bit signed integer attribute declaration.\nMulti-value (there might be multiple attributes declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nNote that unlike sql_attr_uint,\nthese values are signed.\nIntroduced in version 0.9.9-rc1.\nExample:\nsql_attr_bigint = my_bigint_id"
    sql_attr_bool:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-bool'
        multi_value: true
        description: "Boolean attribute declaration.\nMulti-value (there might be multiple attributes declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nEquivalent to sql_attr_uint declaration with a bit count of 1.\nExample:\nsql_attr_bool = is_deleted # will be packed to 1 bit"
    sql_attr_uint:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-attr-uint'
        multi_value: true
        description: "Unsigned integer attribute declaration.\nMulti-value (there might be multiple attributes declared), optional.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nThe column value should fit into 32-bit unsigned integer range.\nValues outside this range will be accepted but wrapped around.\nFor instance, -1 will be wrapped around to 2^32-1 or 4,294,967,295.\n\nYou can specify bit count for integer attributes by appending\n':BITCOUNT' to attribute name (see example below).  Attributes with\nless than default 32-bit size, or bitfields, perform slower.\nBut they require less RAM when using extern storage:\nsuch bitfields are packed together in 32-bit chunks in .spa\nattribute data file. Bit size settings are ignored if using\ninline storage.\nExample:\nsql_attr_uint = group_id\nsql_attr_uint = forum_id:9 # 9 bits for forum_id"
    sql_query_killlist:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-query-killlist'
        multi_value: false
        description: "Kill-list query.\nOptional, default is empty (no query).\nApplies to SQL source types (mysql, pgsql, mssql) only.\nIntroduced in version 0.9.9-rc1.\n\nThis query is expected to return a number of 1-column rows, each containing\njust the document ID. The returned document IDs are stored within an index.\nKill-list for a given index suppresses results from other\nindexes, depending on index order in the query. The intended use is to help\nimplement deletions and updates on existing indexes without rebuilding\n(actually even touching them), and especially to fight phantom results\nproblem.\n\nLet us dissect an example. Assume we have two indexes, 'main' and 'delta'.\nAssume that documents 2, 3, and 5 were deleted since last reindex of 'main',\nand documents 7 and 11 were updated (ie. their text contents were changed).\nAssume that a keyword 'test' occurred in all these mentioned documents\nwhen we were indexing 'main'; still occurs in document 7 as we index 'delta';\nbut does not occur in document 11 any more. We now reindex delta and then\nsearch through both these indexes in proper (least to most recent) order:\n\n$res = $cl->Query ( \"test\", \"main delta\" );\n\n\nFirst, we need to properly handle deletions. The result set should not\ncontain documents 2, 3, or 5. Second, we also need to avoid phantom results.\nUnless we do something about it, document 11 will\nappear in search results! It will be found in 'main' (but not 'delta').\nAnd it will make it to the final result set unless something stops it.\n\nKill-list, or K-list for short, is that something. Kill-list attached\nto 'delta' will suppress the specified rows from all the preceding\nindexes, in this case just 'main'. So to get the expected results,\nwe should put all the updated and deleted\ndocument IDs into it.\n\nNote that in the distributed index setup, K-lists are local\nto every node in the cluster. They are not get transmitted\nover the network when sending queries. (Because that might be too much\nof an impact when the K-list is huge.) You will need to setup a\nseparate per-server K-lists in that case.\nExample:\nsql_query_killlist = \\\n    SELECT id FROM documents WHERE updated_ts>=@last_reindex UNION \\\n    SELECT id FROM documents_deleted WHERE deleted_ts>=@last_reindex"
    sql_range_step:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-range-step'
        multi_value: false
        description: "Range query step.\nOptional, default is 1024.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nOnly used when ranged queries are enabled.\nThe full document IDs interval fetched by sql_query_range\nwill be walked in this big steps. For example, if min and max IDs fetched\nare 12 and 3456 respectively, and the step is 1000, indexer will call\nsql_query several times with the\nfollowing substitutions:\n$start=12, $end=1011\n$start=1012, $end=2011\n$start=2012, $end=3011\n$start=3012, $end=3456\n\n\nExample:\nsql_range_step = 1000"
    sql_query_range:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-query-range'
        multi_value: false
        description: "Range query setup.\nOptional, default is empty.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nSetting this option enables ranged document fetch queries (see Section\_3.8, “Ranged queries”).\nRanged queries are useful to avoid notorious MyISAM table locks when indexing\nlots of data. (They also help with other less notorious issues, such as reduced\nperformance caused by big result sets, or additional resources consumed by InnoDB\nto serialize big read transactions.)\n\nThe query specified in this option must fetch min and max document IDs that will be\nused as range boundaries. It must return exactly two integer fields, min ID first\nand max ID second; the field names are ignored.\n\nWhen ranged queries are enabled, sql_query\nwill be required to contain $start and $end macros\n(because it obviously would be a mistake to index the whole table many times over).\nNote that the intervals specified by $start..$end\nwill not overlap, so you should not remove document IDs that are\nexactly equal to $start or $end from your query.\nThe example in Section\_3.8, “Ranged queries”) illustrates that; note how it\nuses greater-or-equal and less-or-equal comparisons.\nExample:\nsql_query_range = SELECT MIN(id),MAX(id) FROM documents"
    sql_query:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-query'
        multi_value: false
        description: "Main document fetch query.\nMandatory, no default value.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nThere can be only one main query.\nThis is the query which is used to retrieve documents from SQL server.\nYou can specify up to 32 full-text fields (formally, upto SPH_MAX_FIELDS from sphinx.h), and an arbitrary amount of attributes.\nAll of the columns that are neither document ID (the first one) nor attributes will be full-text indexed.\n\nDocument ID MUST be the very first field,\nand it MUST BE UNIQUE UNSIGNED POSITIVE (NON-ZERO, NON-NEGATIVE) INTEGER NUMBER.\nIt can be either 32-bit or 64-bit, depending on how you built Sphinx;\nby default it builds with 32-bit IDs support but --enable-id64 option\nto configure allows to build with 64-bit document and word IDs support.\n\nExample:\nsql_query = \\\n    SELECT id, group_id, UNIX_TIMESTAMP(date_added) AS date_added, \\\n        title, content \\\n    FROM documents"
    sql_query_pre:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-query-pre'
        multi_value: true
        description: "Pre-fetch query, or pre-query.\nMulti-value, optional, default is empty list of queries.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nMulti-value means that you can specify several pre-queries.\nThey are executed before the main fetch query,\nand they will be executed exactly in order of appearance in the configuration file.\nPre-query results are ignored.\n\nPre-queries are useful in a lot of ways. They are used to setup encoding,\nmark records that are going to be indexed, update internal counters,\nset various per-connection SQL server options and variables, and so on.\n\nPerhaps the most frequent pre-query usage is to specify the encoding\nthat the server will use for the rows it returns. Note that Sphinx accepts\nonly UTF-8 texts.\nTwo MySQL specific examples of setting the encoding are:\n\nsql_query_pre = SET CHARACTER_SET_RESULTS=utf8\nsql_query_pre = SET NAMES utf8\n\nAlso specific to MySQL sources, it is useful to disable query cache\n(for indexer connection only) in pre-query, because indexing queries\nare not going to be re-run frequently anyway, and there's no sense\nin caching their results. That could be achieved with:\n\nsql_query_pre = SET SESSION query_cache_type=OFF\n\nExample:\nsql_query_pre = SET NAMES utf8\nsql_query_pre = SET SESSION query_cache_type=OFF"
    odbc_dsn:
        link: 'http://sphinxsearch.com/docs/current.html#conf-odbc-dsn'
        multi_value: false
        description: "ODBC DSN to connect to.\nMandatory, no default value.\nApplies to odbc source type only.\n\nODBC DSN (Data Source Name) specifies the credentials (host, user, password, etc)\nto use when connecting to ODBC data source. The format depends on specific ODBC\ndriver used.\nExample:\nodbc_dsn = Driver={Oracle ODBC Driver};Dbq=myDBName;Uid=myUsername;Pwd=myPassword"
    mysql_ssl_cert:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mysql-ssl'
        multi_value: false
        description: "SSL certificate settings to use for connecting to MySQL server.\nOptional, default values are empty strings (do not use SSL).\nApplies to mysql source type only.\n\nThese directives let you set up secure SSL connection between\nindexer and MySQL. The details on creating\nthe certificates and setting up MySQL server can be found in\nMySQL documentation.\nExample:\nmysql_ssl_cert = /etc/ssl/client-cert.pem\nmysql_ssl_key = /etc/ssl/client-key.pem\nmysql_ssl_ca = /etc/ssl/cacert.pem"
    mysql_ssl_key:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mysql-ssl'
        multi_value: false
        description: "SSL certificate settings to use for connecting to MySQL server.\nOptional, default values are empty strings (do not use SSL).\nApplies to mysql source type only.\n\nThese directives let you set up secure SSL connection between\nindexer and MySQL. The details on creating\nthe certificates and setting up MySQL server can be found in\nMySQL documentation.\nExample:\nmysql_ssl_cert = /etc/ssl/client-cert.pem\nmysql_ssl_key = /etc/ssl/client-key.pem\nmysql_ssl_ca = /etc/ssl/cacert.pem"
    mysql_ssl_ca:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mysql-ssl'
        multi_value: false
        description: "SSL certificate settings to use for connecting to MySQL server.\nOptional, default values are empty strings (do not use SSL).\nApplies to mysql source type only.\n\nThese directives let you set up secure SSL connection between\nindexer and MySQL. The details on creating\nthe certificates and setting up MySQL server can be found in\nMySQL documentation.\nExample:\nmysql_ssl_cert = /etc/ssl/client-cert.pem\nmysql_ssl_key = /etc/ssl/client-key.pem\nmysql_ssl_ca = /etc/ssl/cacert.pem"
    mysql_connect_flags:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mysql-connect-flags'
        multi_value: false
        description: "MySQL client connection flags.\nOptional, default value is 0 (do not set any flags).\nApplies to mysql source type only.\n\nThis option must contain an integer value with the sum of the flags.\nThe value will be passed to mysql_real_connect() verbatim.\nThe flags are enumerated in mysql_com.h include file.\nFlags that are especially interesting in regard to indexing, with their respective values, are as follows:\nCLIENT_COMPRESS = 32; can use compression protocol\nCLIENT_SSL = 2048; switch to SSL after handshake\nCLIENT_SECURE_CONNECTION = 32768; new 4.1 authentication\n\n\nFor instance, you can specify 2080 (2048+32) to use both compression and SSL,\nor 32768 to use new authentication only. Initially, this option was introduced\nto be able to use compression when the indexer\nand mysqld are on different hosts. Compression on 1 Gbps\nlinks is most likely to hurt indexing time though it reduces network traffic,\nboth in theory and in practice. However, enabling compression on 100 Mbps links\nmay improve indexing time significantly (upto 20-30% of the total indexing time\nimprovement was reported). Your mileage may vary.\nExample:\nmysql_connect_flags = 32 # enable compression"
    sql_sock:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-sock'
        multi_value: false
        description: "UNIX socket name to connect to for local SQL servers.\nOptional, default value is empty (use client library default settings).\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nOn Linux, it would typically be /var/lib/mysql/mysql.sock.\nOn FreeBSD, it would typically be /tmp/mysql.sock.\nNote that it depends on sql_host setting whether this value will actually be used.\nExample:\nsql_sock = /tmp/mysql.sock"
    sql_db:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-db'
        multi_value: false
        description: "SQL database (in MySQL terms) to use after the connection and perform further queries within.\nMandatory, no default value.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nExample:\nsql_db = test"
    sql_pass:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-pass'
        multi_value: false
        description: "SQL user password to use when connecting to sql_host.\nMandatory, no default value.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nExample:\nsql_pass = mysecretpassword"
    sql_user:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-user'
        multi_value: false
        description: "SQL user to use when connecting to sql_host.\nMandatory, no default value.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nExample:\nsql_user = test"
    sql_port:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-port'
        multi_value: false
        description: "SQL server IP port to connect to.\nOptional, default is 3306 for mysql source type and 5432 for pgsql type.\nApplies to SQL source types (mysql, pgsql, mssql) only.\nNote that it depends on sql_host setting whether this value will actually be used.\nExample:\nsql_port = 3306"
    sql_host:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sql-host'
        multi_value: false
        description: "SQL server host to connect to.\nMandatory, no default value.\nApplies to SQL source types (mysql, pgsql, mssql) only.\n\nIn the simplest case when Sphinx resides on the same host with your MySQL\nor PostgreSQL installation, you would simply specify \"localhost\". Note that\nMySQL client library chooses whether to connect over TCP/IP or over UNIX\nsocket based on the host name. Specifically \"localhost\" will force it\nto use UNIX socket (this is the default and generally recommended mode)\nand \"127.0.0.1\" will force TCP/IP usage. Refer to\nMySQL manual\nfor more details.\nExample:\nsql_host = localhost"
    type:
        link: 'http://sphinxsearch.com/docs/current.html#conf-source-type'
        multi_value: false
        description: "Data source type.\nMandatory, no default value.\nKnown types are mysql, pgsql, mssql,\nxmlpipe2, tsvpipe, csvpipe and odbc.\n\nAll other per-source options depend on source type selected by this option.\nNames of the options used for SQL sources (ie. MySQL, PostgreSQL, MS SQL) start with \"sql_\";\nnames of the ones used for xmlpipe2 or tsvpipe, csvpipe start with \"xmlpipe_\" and \"tsvpipe_\",\n\"csvpipe_\" correspondingly.\nAll source types are conditional; they might or might\nnot be supported depending on your build settings, installed client libraries, etc.\nmssql type is currently only available on Windows.\nodbc type is available both on Windows natively and on\nLinux through UnixODBC library.\nExample:\ntype = mysql"
index:
    ondisk_attrs:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ondisk-attrs'
        multi_value: false
        description: "Allows for fine-grain control over how attributes are loaded into memory\nwhen using indexes with external storage. It is now possible (since\nversion 2.2.1-beta) to keep attributes on disk. Although, the daemon does\nmap them to memory and the OS loads small chunks of data on demand. This\nallows use of docinfo = extern instead of docinfo = inline, but still\nleaves plenty of free memory for cases when you have large collections\nof pooled attributes (string/JSON/MVA) or when you're using many indexes\nper daemon that don't consume memory. It is not possible to update\nattributes left on disk when this option is enabled and the constraint\nof 4Gb of entries per pool is still in effect.\n\nNote that this option also affects RT indexes. When it is enabled, all atribute updates\nwill be disabled, and also all disk chunks of RT indexes will behave described above. However\ninserting and deleting of docs from RT indexes is still possible with enabled ondisk_attrs.\nPossible values:\n0 - disabled and default value, all attributes are loaded in memory\n(the normal behaviour of docinfo = extern)\n\n\n1 - all attributes stay on disk. Daemon loads no files (spa, spm, sps).\nThis is the most memory conserving mode, however it is also the slowest\nas the whole doc-id-list and block index doesn't load.\n\n\npool - only pooled attributes stay on disk. Pooled attributes are string,\nMVA, and JSON attributes (sps, spm files). Scalar attributes stored in\ndocinfo (spa file) load as usual.\n\n\n\nThis option does not affect indexing in any way, it only requires daemon\nrestart.\nExample:\nondisk_attrs = pool #keep pooled attributes on disk"
    rlp_context:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rlp-context'
        multi_value: false
        description: "RLP context configuration file. Mandatory if RLP is used.\nAdded in 2.2.1-beta.\nExample:\nrlp_context = /home/myuser/RLP/rlp-context.xml"
    global_idf:
        link: 'http://sphinxsearch.com/docs/current.html#conf-global-idf'
        multi_value: false
        description: "The path to a file with global (cluster-wide) keyword IDFs.\nOptional, default is empty (use local IDFs).\nAdded in 2.1.1-beta.\n\nOn a multi-index cluster, per-keyword frequencies are quite\nlikely to differ across different indexes. That means that when\nthe ranking function uses TF-IDF based values, such as BM25 family\nof factors, the results might be ranked slightly different\ndepending on what cluster node they reside.\n\nThe easiest way to fix that issue is to create and utilize\na global frequency dictionary, or a global IDF file for short.\nThis directive lets you specify the location of that file.\nIt it suggested (but not required) to use a .idf extension.\nWhen the IDF file is specified for a given index and\nand OPTION global_idf is set to 1, the engine will use the keyword\nfrequencies and collection documents count from the global_idf file,\nrather than just the local index. That way, IDFs and the values\nthat depend on them will stay consistent across the cluster.\n\nIDF files can be shared across multiple indexes. Only a single\ncopy of an IDF file will be loaded by searchd,\neven when many indexes refer to that file. Should the contents of\nan IDF file change, the new contents can be loaded with a SIGHUP.\n\nYou can build an .idf file using indextool\nutility, by dumping dictionaries using --dumpdict switch\nfirst, then converting those to .idf format using --buildidf,\nthen merging all .idf files across cluser using --mergeidf.\nRefer to Section\_7.4, “indextool command reference” for more information.\nExample:\nglobal_idf = /usr/local/sphinx/var/global.idf"
    stopwords_unstemmed:
        link: 'http://sphinxsearch.com/docs/current.html#conf-stopwords-unstemmed'
        multi_value: false
        description: "Whether to apply stopwords before or after stemming.\nOptional, default is 0 (apply stopword filter after stemming).\nAdded in 2.1.1-beta.\n\nBy default, stopwords are stemmed themselves, and applied to\ntokens after stemming (or any other morphology\nprocessing). In other words, by default, a token is stopped when\nstem(token) == stem(stopword). That can lead to unexpected results\nwhen a token gets (erroneously) stemmed to a stopped root. For example,\n'Andes' gets stemmed to 'and' by our current stemmer implementation,\nso when 'and' is a stopword, 'Andes' is also stopped.\n\nstopwords_unstemmed directive fixes that issue. When it's enabled,\nstopwords are applied before stemming (and therefore to the original\nword forms), and the tokens are stopped when token == stopword.\nExample:\nstopwords_unstemmed = 1"
    regexp_filter:
        link: 'http://sphinxsearch.com/docs/current.html#conf-regexp-filter'
        multi_value: true
        description: "Regular expressions (regexps) to filter the fields and queries with.\nOptional, multi-value, default is an empty list of regexps.\nAdded in 2.1.1-beta.\n\nIn certain applications (like product search) there can be\nmany different ways to call a model, or a product, or a property,\nand so on. For instance, 'iphone 3gs' and 'iphone 3 gs'\n(or even 'iphone3 gs') are very likely to mean the same\nproduct. Or, for a more tricky example, '13-inch', '13 inch',\n'13\"', and '13in' in a laptop screen size descriptions do mean\nthe same.\n\nRegexps provide you with a mechanism to specify a number of rules\nspecific to your application to handle such cases. In the first\n'iphone 3gs' example, you could possibly get away with a wordforms\nfiles tailored to handle a handful of iPhone models. However even\nin a comparatively simple second '13-inch' example there is just\nway too many individual forms and you are better off specifying\nrules that would normalize both '13-inch' and '13in' to something\nidentical.\n\nRegular expressions listed in regexp_filter are\napplied in the order they are listed. That happens at the earliest\nstage possible, before any other processing, even before tokenization.\nThat is, regexps are applied to the raw source fields when indeixng,\nand to the raw search query text when searching.\n\nWe use the RE2 engine\nto implement regexps. So when building from the source, the library must be\ninstalled in the system and Sphinx must be configured built with a\n--with-re2 switch. Binary packages should come with RE2\nbuiltin.\nExample:\n# index '13-inch' as '13inch'\nregexp_filter = \\b(\\d+)\\\" => \\1inch\n\n# index 'blue' or 'red' as 'color'\nregexp_filter = (blue|red) => color"
    index_field_lengths:
        link: 'http://sphinxsearch.com/docs/current.html#conf-index-field-lengths'
        multi_value: false
        description: "Enables computing and storing of field lengths (both per-document and\naverage per-index values) into the index.\nOptional, default is 0 (do not compute and store).\nAdded in 2.1.1-beta.\n\nWhen index_field_lengths is set to 1, indexer\nwill 1) create a respective length attribute for every full-text field,\nsharing the same name but with _len suffix; 2) compute a field length (counted in keywords) for\nevery document and store in to a respective attribute; 3) compute the per-index\naverages. The lengths attributes will have a special TOKENCOUNT type, but their\nvalues are in fact regular 32-bit integers, and their values are generally\naccessible.\n\nBM25A() and BM25F() functions in the expression ranker are based\non these lengths and require index_field_lengths to be enabled.\nHistorically, Sphinx used a simplified, stripped-down variant of BM25 that,\nunlike the complete function, did not account for document length.\n(We later realized that it should have been called BM15 from the start.)\nStarting with 2.1.1-beta, we added support for both a complete variant of BM25,\nand its extension towards multiple fields, called BM25F. They require\nper-document length and per-field lengths, respectively. Hence the additional\ndirective.\nExample:\nindex_field_lengths = 1"
    bigram_index:
        link: 'http://sphinxsearch.com/docs/current.html#conf-bigram-index'
        multi_value: false
        description: "Bigram indexing mode.\nOptional, default is none.\nAdded in 2.1.1-beta.\n\nBigram indexing is a feature to accelerate phrase searches.\nWhen indexing, it stores a document list for either all or some\nof the adjacent words pairs into the index. Such a list can then be used\nat searching time to significantly accelerate phrase or sub-phrase\nmatching.\n\nbigram_index controls the selection of specific word pairs.\nThe known modes are:\nall, index every single word pair.\n(NB: probably totally not worth it even on a moderately sized index,\nbut added anyway for the sake of completeness.)\n\nfirst_freq, only index word pairs\nwhere the first word is in a list of frequent words\n(see Section\_12.2.61, “bigram_freq_words”). For example, with\nbigram_freq_words = the, in, i, a, indexing\n\"alone in the dark\" text will result in \"in the\" and \"the dark\" pairs\nbeing stored as bigrams, because they begin with a frequent keyword\n(either \"in\" or \"the\" respectively), but \"alone in\" would not\nbe indexed, because \"in\" is a second word in that pair.\n\nboth_freq, only index word pairs where\nboth words are frequent. Continuing with the same example, in this mode\nindexing \"alone in the dark\" would only store \"in the\" (the very worst\nof them all from searching perspective) as a bigram, but none of the\nother word pairs.\n\n\n\nFor most usecases, both_freq would be the best mode, but\nyour mileage may vary.\nExample:\nbigram_freq_words = both_freq"
    bigram_freq_words:
        link: 'http://sphinxsearch.com/docs/current.html#conf-bigram-freq-words'
        multi_value: false
        description: "A list of keywords considered \"frequent\" when indexing bigrams.\nOptional, default is empty.\nAdded in 2.1.1-beta.\n\nBigram indexing is a feature to accelerate phrase searches.\nWhen indexing, it stores a document list for either all or some\nof the adjacent words pairs into the index. Such a list can then be used\nat searching time to significantly accelerate phrase or sub-phrase\nmatching.\n\nSome of the bigram indexing modes (see Section\_12.2.62, “bigram_index”)\nrequire to define a list of frequent keywords. These are not to be\nconfused with stopwords! Stopwords are completely eliminated when both indexing\nand searching. Frequent keywords are only used by bigrams to determine whether\nto index a current word pair or not.\n\nbigram_freq_words lets you define a list of such keywords.\nExample:\nbigram_freq_words = the, a, you, i"
    ha_strategy:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ha-strategy'
        multi_value: false
        description: "Agent mirror selection strategy, for load balancing.\nOptional, default is random.\nAdded in 2.1.1-beta.\n\nThe strategy used for mirror selection, or in other words, choosing\na specific agent mirror in a distributed\nindex. Essentially, this directive controls how exactly master does the\nload balancing between the configured mirror agent nodes.\nAs of 2.1.1-beta, the following strategies are implemented:\nSimple random balancingha_strategy = random\nThe default balancing mode. Simple linear random distribution among the mirrors.\nThat is, equal selection probability are assigned to every mirror. Kind of similar\nto round-robin (RR), but unlike RR, does not impose a strict selection order.\nAdaptive randomized balancing\nThe default simple random strategy does not take mirror status, error rate,\nand, most importantly, actual response latencies into account. So to accommodate\nfor heterogeneous clusters and/or temporary spikes in agent node load, we have\na group of balancing strategies that dynamically adjusts the probabilities\nbased on the actual query latencies observed by the master.\n\nThe adaptive strategies based on latency-weighted probabilities\nbasically work as follows:\nlatency stats are accumulated, in blocks of ha_period_karma seconds;\nonce per karma period, latency-weighted probabilities get recomputed;\nonce per request (including ping requests), \"dead or alive\" flag is adjusted.\n\n\nCurrently (as of 2.1.1-beta), we begin with equal probabilities (or percentages,\nfor brevity), and on every step, scale them by the inverse of the latencies observed\nduring the last \"karma\" period, and then renormalize them.  For example, if during\nthe first 60 seconds after the master startup 4 mirrors had latencies of\n10, 5, 30, and 3 msec/query respectively, the first adjustment step\nwould go as follow:\ninitial percentages: 0.25, 0.25, 0.25, 0.2%;\nobserved latencies: 10 ms, 5 ms, 30 ms, 3 ms;\ninverse latencies: 0.1, 0.2, 0.0333, 0.333;\nscaled percentages: 0.025, 0.05, 0.008333, 0.0833;\nrenormalized percentages: 0.15, 0.30, 0.05, 0.50.\n\n\nMeaning that the 1st mirror would have a 15% chance of being chosen during\nthe next karma period, the 2nd one a 30% chance, the 3rd one (slowest at 30 ms)\nonly a 5% chance, and the 4th and the fastest one (at 3 ms) a 50% chance.\nThen, after that period, the second adjustment step would update those chances\nagain, and so on.\n\nThe rationale here is, once the observed latencies stabilize,\nthe latency weighted probabilities stabilize as well. So all these\nadjustment iterations are supposed to converge at a point where the average\nlatencies are (roughly) equal over all mirrors.\nha_strategy = nodeads\nLatency-weighted probabilities, but dead mirrors are excluded from\nthe selection. \"Dead\" mirror is defined as a mirror that resulted\nin multiple hard errors (eg. network failure, or no answer, etc) in a row.\nha_strategy = noerrors\nLatency-weighted probabilities, but mirrors with worse errors/success ratio\nare excluded from the selection.\nRound-robin balancingha_strategy = roundrobinSimple round-robin selection, that is, selecting the 1st mirror\nin the list, then the 2nd one, then the 3rd one, etc, and then repeating\nthe process once the last mirror in the list is reached. Unlike with\nthe randomized strategies, RR imposes a strict querying order (1, 2, 3, ..,\nN-1, N, 1, 2, 3, ... and so on) and guarantees that\nno two subsequent queries will be sent to the same mirror."
    rt_attr_json:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-json'
        multi_value: true
        description: "JSON attribute declaration.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nIntroduced in version 2.1.1-beta.\n\nRefer to Section\_12.1.24, “sql_attr_json” for more details on the JSON attributes.\nExample:\nrt_attr_json = properties"
    rt_attr_string:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-string'
        multi_value: true
        description: "String attribute declaration.\nMulti-value (an arbitrary number of attributes is allowed), optional.\nIntroduced in version 1.10-beta.\nExample:\nrt_attr_string = author"
    rt_attr_timestamp:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-timestamp'
        multi_value: true
        description: "Timestamp attribute declaration.\nMulti-value (an arbitrary number of attributes is allowed), optional.\nIntroduced in version 1.10-beta.\nExample:\nrt_attr_timestamp = date_added"
    rt_attr_multi_64:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-multi-64'
        multi_value: true
        description: "Multi-valued attribute (MVA) declaration.\nDeclares the BIGINT (signed 64-bit) MVA attribute.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nApplies to RT indexes only.\nExample:\nrt_attr_multi_64 = my_wide_tags"
    rt_attr_multi:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-multi'
        multi_value: true
        description: "Multi-valued attribute (MVA) declaration.\nDeclares the UNSIGNED INTEGER (unsigned 32-bit) MVA attribute.\nMulti-value (ie. there may be more than one such attribute declared), optional.\nApplies to RT indexes only.\nExample:\nrt_attr_multi = my_tags"
    rt_attr_float:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-float'
        multi_value: true
        description: "Floating point attribute declaration.\nMulti-value (an arbitrary number of attributes is allowed), optional.\nDeclares a single precision, 32-bit IEEE 754 format float attribute.\nIntroduced in version 1.10-beta.\nExample:\nrt_attr_float = gpa"
    rt_attr_bigint:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-bigint'
        multi_value: true
        description: "BIGINT attribute declaration.\nMulti-value (an arbitrary number of attributes is allowed), optional.\nDeclares a signed 64-bit attribute.\nIntroduced in version 1.10-beta.\nExample:\nrt_attr_bigint = guid"
    rt_attr_bool:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-bool'
        multi_value: true
        description: "Boolean attribute declaration.\nMulti-value (there might be multiple attributes declared), optional.\nDeclares a 1-bit unsigned integer attribute.\nIntroduced in version 2.1.2-release.\nExample:\nrt_attr_bool = available"
    rt_attr_uint:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-attr-uint'
        multi_value: true
        description: "Unsigned integer attribute declaration.\nMulti-value (an arbitrary number of attributes is allowed), optional.\nDeclares an unsigned 32-bit attribute.\nIntroduced in version 1.10-beta.\nExample:\nrt_attr_uint = gid"
    rt_field:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-field'
        multi_value: true
        description: "Full-text field declaration.\nMulti-value, mandatory\nIntroduced in version 1.10-beta.\n\nFull-text fields to be indexed are declared using rt_field\ndirective. The names must be unique. The order is preserved; and so field values\nin INSERT statements without an explicit list of inserted columns will have to be\nin the same order as configured.\n\nExample:\nrt_field = author\nrt_field = title\nrt_field = content"
    rt_mem_limit:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-mem-limit'
        multi_value: false
        description: "RAM chunk size limit.\nOptional, default is 128M.\nIntroduced in version 1.10-beta.\n\nRT index keeps some data in memory (so-called RAM chunk) and\nalso maintains a number of on-disk indexes (so-called disk chunks).\nThis directive lets you control the RAM chunk size. Once there's\ntoo much data to keep in RAM, RT index will flush it to disk,\nactivate a newly created disk chunk, and reset the RAM chunk.\n\nThe limit is pretty strict; RT index should never allocate more\nmemory than it's limited to. The memory is not preallocated either,\nhence, specifying 512 MB limit and only inserting 3 MB of data\nshould result in allocating 3 MB, not 512 MB.\n\nExample:\nrt_mem_limit = 512M"
    blend_mode:
        link: 'http://sphinxsearch.com/docs/current.html#conf-blend-mode'
        multi_value: false
        description: "Blended tokens indexing mode.\nOptional, default is trim_none.\nIntroduced in version 2.0.1-beta.\n\nBy default, tokens that mix blended and non-blended characters\nget indexed in there entirety. For instance, when both at-sign and\nan exclamation are in blend_chars, \"@dude!\" will get\nresult in two tokens indexed: \"@dude!\" (with all the blended characters)\nand \"dude\" (without any). Therefore \"@dude\" query will not\nmatch it.\n\nblend_mode directive adds flexibility to this indexing\nbehavior. It takes a comma-separated list of options.\n\nblend_mode = option [, option [, ...]]\noption = trim_none | trim_head | trim_tail | trim_both | skip_pure\n\n\nOptions specify token indexing variants. If multiple options are\nspecified, multiple variants of the same token will be indexed.\nRegular keywords (resulting from that token by replacing blended\nwith whitespace) are always be indexed.\ntrim_none\nIndex the entire token.trim_head\nTrim heading blended characters, and index the resulting token.trim_tail\nTrim trailing blended characters, and index the resulting token.trim_both\nTrim both heading and trailing blended characters, and index the resulting token.skip_pure\nDo not index the token if it's purely blended, that is, consists of blended characters only.\n\nReturning to the \"@dude!\" example above, setting blend_mode = trim_head,\ntrim_tail will result in two tokens being indexed, \"@dude\" and \"dude!\".\nIn this particular example, trim_both would have no effect,\nbecause trimming both blended characters results in \"dude\" which is already\nindexed as a regular keyword. Indexing \"@U.S.A.\" with trim_both\n(and assuming that dot is blended two) would result in \"U.S.A\" being indexed.\nLast but not least, skip_pure enables you to fully ignore\nsequences of blended characters only. For example, \"one @@@ two\" would be\nindexed exactly as \"one two\", and match that as a phrase. That is not the case\nby default because a fully blended token gets indexed and offsets the second\nkeyword position.\n\nDefault behavior is to index the entire token, equivalent to\nblend_mode = trim_none.\nExample:\nblend_mode = trim_tail, skip_pure"
    blend_chars:
        link: 'http://sphinxsearch.com/docs/current.html#conf-blend-chars'
        multi_value: false
        description: "Blended characters list.\nOptional, default is empty.\nIntroduced in version 1.10-beta.\n\nBlended characters are indexed both as separators and valid characters.\nFor instance, assume that & is configured as blended and AT&T\noccurs in an indexed document. Three different keywords will get indexed,\nnamely \"at&t\", treating blended characters as valid, plus \"at\" and \"t\",\ntreating them as separators.\n\nPositions for tokens obtained by replacing blended characters with whitespace\nare assigned as usual, so regular keywords will be indexed just as if there was\nno blend_chars specified at all. An additional token that\nmixes blended and non-blended characters will be put at the starting position.\nFor instance, if the field contents are \"AT&T company\" occurs in the very\nbeginning of the text field, \"at\" will be given position 1, \"t\" position 2,\n\"company\" position 3, and \"AT&T\" will also be given position 1 (\"blending\"\nwith the opening regular keyword). Thus, querying for either AT&T or just\nAT will match that document, and querying for \"AT T\" as a phrase also match it.\nLast but not least, phrase query for \"AT&T company\" will also\nmatch it, despite the position\n\nBlended characters can overlap with special characters used in query\nsyntax (think of T-Mobile or @twitter). Where possible, query parser will\nautomatically handle blended character as blended. For instance, \"hello @twitter\"\nwithin quotes (a phrase operator) would handle @-sign as blended, because\n@-syntax for field operator is not allowed within phrases. Otherwise,\nthe character would be handled as an operator. So you might want to\nescape the keywords.\n\nStarting with version 2.0.1-beta, blended characters can be remapped,\nso that multiple different blended characters could be normalized into\njust one base form. This is useful when indexing multiple alternative\nUnicode codepoints with equivalent glyphs.\nExample:\nblend_chars = +, &, U+23\nblend_chars = +, &->+ # 2.0.1 and above"
    expand_keywords:
        link: 'http://sphinxsearch.com/docs/current.html#conf-expand-keywords'
        multi_value: false
        description: "Expand keywords with exact forms and/or stars when possible.\nOptional, default is 0 (do not expand keywords).\nIntroduced in version 1.10-beta.\n\nQueries against indexes with expand_keywords feature\nenabled are internally expanded as follows. If the index was built with\nprefix or infix indexing enabled, every keyword gets internally replaced\nwith a disjunction of keyword itself and a respective prefix or infix\n(keyword with stars). If the index was built with both stemming and\nindex_exact_words enabled,\nexact form is also added. Here's an example that shows how internal\nexpansion works when all of the above (infixes, stemming, and exact\nwords) are combined:\n\nrunning -> ( running | *running* | =running )\n\n\nExpanded queries take naturally longer to complete, but can possibly\nimprove the search quality, as the documents with exact form matches\nshould be ranked generally higher than documents with stemmed or infix matches.\n\nNote that the existing query syntax does not allow to emulate this\nkind of expansion, because internal expansion works on keyword level and\nexpands keywords within phrase or quorum operators too (which is not\npossible through the query syntax).\n\nThis directive does not affect indexer in any way,\nit only affects searchd.\nExample:\nexpand_keywords = 1"
    stopword_step:
        link: 'http://sphinxsearch.com/docs/current.html#conf-stopword-step'
        multi_value: false
        description: "Position increment on stopwords.\nOptional, allowed values are 0 and 1, default is 1.\nIntroduced in version 0.9.9-rc1.\n\nThis directive does not affect searchd in any way,\nit only affects indexer.\nExample:\nstopword_step = 1"
    overshort_step:
        link: 'http://sphinxsearch.com/docs/current.html#conf-overshort-step'
        multi_value: false
        description: "Position increment on overshort (less that min_word_len) keywords.\nOptional, allowed values are 0 and 1, default is 1.\nIntroduced in version 0.9.9-rc1.\n\nThis directive does not affect searchd in any way,\nit only affects indexer.\nExample:\novershort_step = 1"
    index_exact_words:
        link: 'http://sphinxsearch.com/docs/current.html#conf-index-exact-words'
        multi_value: false
        description: "Whether to index the original keywords along with the stemmed/remapped versions.\nOptional, default is 0 (do not index).\nIntroduced in version 0.9.9-rc1.\n\nWhen enabled, index_exact_words forces indexer\nto put the raw keywords in the index along with the stemmed versions. That, in turn,\nenables exact form operator in the query language to work.\nThis impacts the index size and the indexing time. However, searching performance\nis not impacted at all.\nExample:\nindex_exact_words = 1"
    preopen:
        link: 'http://sphinxsearch.com/docs/current.html#conf-preopen'
        multi_value: false
        description: "Whether to pre-open all index files, or open them per each query.\nOptional, default is 0 (do not preopen).\n\nThis option tells searchd that it should pre-open\nall index files on startup (or rotation) and keep them open while it runs.\nCurrently, the default mode is not to pre-open the files (this may\nchange in the future). Preopened indexes take a few (currently 2) file\ndescriptors per index. However, they save on per-query open() calls;\nand also they are invulnerable to subtle race conditions that may happen during\nindex rotation under high load. On the other hand, when serving many indexes\n(100s to 1000s), it still might be desired to open the on per-query basis\nin order to save file descriptors.\n\nThis directive does not affect indexer in any way,\nit only affects searchd.\nExample:\npreopen = 1"
    agent_query_timeout:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-query-timeout'
        multi_value: false
        description: "Remote agent query timeout, in milliseconds.\nOptional, default is 3000 (ie. 3 seconds).\nAdded in version 2.1.1-beta.\n\nAfter connection, searchd will wait at most this\nmuch time for remote queries to complete. This timeout is fully separate\nfrom connection timeout; so the maximum possible delay caused by\na remote agent equals to the sum of agent_connection_timeout and\nagent_query_timeout. Queries will not be retried\nif this timeout is reached; a warning will be produced instead.\nExample:\nagent_query_timeout = 10000 # our query can be long, allow up to 10 sec"
    agent_connect_timeout:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-connect-timeout'
        multi_value: false
        description: "Remote agent connection timeout, in milliseconds.\nOptional, default is 1000 (ie. 1 second).\n\nWhen connecting to remote agents, searchd\nwill wait at most this much time for connect() call to complete\nsuccessfully. If the timeout is reached but connect() does not complete,\nand retries are enabled,\nretry will be initiated.\nExample:\nagent_connect_timeout = 300"
    agent_blackhole:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-blackhole'
        multi_value: true
        description: "Remote blackhole agent declaration in the distributed index.\nMulti-value, optional, default is empty.\nIntroduced in version 0.9.9-rc1.\n\nagent_blackhole lets you fire-and-forget queries\nto remote agents. That is useful for debugging (or just testing)\nproduction clusters: you can setup a separate debugging/testing searchd\ninstance, and forward the requests to this instance from your production\nmaster (aggregator) instance without interfering with production work.\nMaster searchd will attempt to connect and query blackhole agent\nnormally, but it will neither wait nor process any responses.\nAlso, all network errors on blackhole agents will be ignored.\nThe value format is completely identical to regular\nagent directive.\nExample:\nagent_blackhole = testbox:9312:testindex1,testindex2"
    agent_persistent:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-persistent'
        multi_value: true
        description: "Persistently connected remote agent declaration.\nMulti-value, optional, default is empty.\nIntroduced in version 2.1.1-beta.\n\nagent_persistent directive syntax matches that of\nthe agent directive. The only difference\nis that the master will not open a new connection to the agent for\nevery query and then close it. Rather, it will keep a connection open and\nattempt to reuse for the subsequent queries. The maximal number of such persistent connections per one agent host\nis limited by persistent_connections_limit option of searchd section.\n\nNote, that you have to set the last one in something greater than 0 if you want to use persistent agent connections.\nOtherwise - when persistent_connections_limit is not defined, it assumes\nthe zero num of persistent connections, and 'agent_persistent' acts exactly as simple 'agent'.\n\nPersistent master-agent connections reduce TCP port pressure, and\nsave on connection handshakes. As of time of this writing, they are supported only\nin workers=threads mode. In other modes, simple non-persistent connections\n(i.e., one connection per operation) will be used, and a warning will show\nup in the console.\nExample:\nagent_persistent = remotebox:9312:index2"
    agent:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent'
        multi_value: true
        description: "Remote agent declaration in the distributed index.\nMulti-value, optional, default is empty.\n\nagent directive declares remote agents that are searched\nevery time when the enclosing distributed index is searched. The agents\nare, essentially, pointers to networked indexes. Prior to version 2.1.1-beta,\nthe value format was:\n\nagent = address:index-list\n\nStarting with 2.1.1-beta, the value can additionally specify multiple\nalternatives (agent mirrors) for either the address only, or the address\nand index list:\n\nagent = address1 [ | address2 [...] ]:index-list\nagent = address1:index-list [ | address2:index-list [...] ]\n\nIn both cases the address specification must be one of the following:\n\naddress = hostname:port # eg. server2:9312\naddress = /absolute/unix/socket/path # eg. /var/run/sphinx2.sock\n\nWhere\nhostname is the remote host name,\nport is the remote TCP port number,\nindex-list is a comma-separated list of index names,\nand square braces [] designate an optional clause.\n\nIn other words, you can point every single agent to one or more remote\nindexes, residing on one or more networked servers. There are absolutely\nno restrictions on the pointers. To point out a couple important things,\nthe host can be localhost, and the remote index can be a distributed\nindex in turn, all that is legal. That enables a bunch of very different\nusage modes:\nsharding over multiple agent servers, and creating\nan arbitrary cluster topology;\nsharding over multiple agent servers, mirrored\nfor HA/LB (High Availability and Load Balancing) purposes\n(starting with 2.1.1-beta);\nsharding within localhost, to utilize multiple cores\n(historical and not recommended in versions 1.x and above, use multiple\nlocal indexes and dist_threads directive instead);\n\n\n\nAll agents are searched in parallel. An index list is passed verbatim\nto the remote agent. How exactly that list is searched within the agent\n(ie. sequentially or in parallel too) depends solely on the agent\nconfiguration (ie. dist_threads directive). Master has no remote\ncontrol over that.\n\nStarting with 2.2.9-release, the value can additionally enumerate per agent\noptions such as:\nha_strategy - random,\nroundrobin, nodeads, noerrors (replces index ha_strategy\nfor particular agent)\nconn - pconn,\npersistent (same as agent_persistent\nagent declaration)\nblackhole - 0,1 (same as\nagent_blackhole agent declaration)\n\n\nagent = address1:index-list[[ha_strategy=value] | [conn=value] | [blackhole=value]]\nExample:\n# config on box2\n# sharding an index over 3 servers\nagent = box2:9312:chunk2\nagent = box3:9312:chunk3\n\n# config on box2\n# sharding an index over 3 servers\nagent = box1:9312:chunk2\nagent = box3:9312:chunk3\n\n# config on box3\n# sharding an index over 3 servers\nagent = box1:9312:chunk2\nagent = box2:9312:chunk3\n\n# per agent options\nagent = box1:9312:chunk1[ha_strategy=nodeads]\nagent = box2:9312:chunk2[conn=pconn]\nagent = test:9312:any[blackhole=1]\nAgent mirrors\nNew syntax added in 2.1.1-beta lets you define so-called agent mirrors\nthat can be used interchangeably when processing a search query. Master server\nkeeps track of mirror status (alive or dead) and response times, and does\nautomatic failover and load balancing based on that. For example, this line:\nagent = box1:9312|box2:9312|box3:9312:chunk2\nDeclares that box1:9312, box2:9312, and box3:9312 all have an index\ncalled chunk2, and can be used as interchangeable mirrors. If any single\nof those servers go down, the queries will be distributed between\nthe other two. When it gets back up, master will detect that and begin\nrouting queries to all three boxes again.\n\nAnother way to define the mirrors is to explicitly specify the index list\nfor every mirror:\nagent = box1:9312:box1chunk2|box2:9312:box2chunk2\nThis works essentially the same as the previous example, but different\nindex names will be used when querying different severs: box1chunk2 when querying\nbox1:9312, and box2chunk when querying box2:9312.\n\nBy default, all queries are routed to the best of the mirrors. The best one\nis picked based on the recent statistics, as controlled by the\nha_period_karma config directive.\nMaster stores a number of metrics (total query count, error count, response\ntime, etc) recently observed for every agent. It groups those by time spans,\nand karma is that time span length. The best agent mirror is then determined\ndynamically based on the last 2 such time spans. Specific algorithm that\nwill be used to pick a mirror can be configured\nha_strategy directive.\n\nThe karma period is in seconds and defaults to 60 seconds. Master stores\nupto 15 karma spans with per-agent statistics for instrumentation purposes\n(see  SHOW AGENT STATUS\nstatement). However, only the last 2 spans out of those are ever used for\nHA/LB logic.\n\nWhen there are no queries, master sends a regular ping command every\nha_ping_interval milliseconds\nin order to have some statistics and at least check, whether the remote\nhost is still alive. ha_ping_interval defaults to 1000 msec. Setting it to 0\ndisables pings and statistics will only be accumulated based on actual queries.\nExample:\n# sharding index over 4 servers total\n# in just 2 chunks but with 2 failover mirrors for each chunk\n# box1, box2 carry chunk1 as local\n# box3, box4 carry chunk2 as local\n\n# config on box1, box2\nagent = box3:9312|box4:9312:chunk2\n\n# config on box3, box4\nagent = box1:9312|box2:9312:chunk1"
    local:
        link: 'http://sphinxsearch.com/docs/current.html#conf-local'
        multi_value: true
        description: "Local index declaration in the distributed index.\nMulti-value, optional, default is empty.\n\nThis setting is used to declare local indexes that will be searched when\ngiven distributed index is searched. Many local indexes can be declared per\neach distributed index. Any local index can also be mentioned several times\nin different distributed indexes.\n\nNote that by default all local indexes will be searched sequentially,\nutilizing only 1 CPU or core. To parallelize processing of the local parts\nin the distributed index, you should use dist_threads directive,\nsee Section\_12.4.24, “dist_threads”.\n\nBefore dist_threads, there also was a legacy solution\nto configure searchd to query itself instead of using\nlocal indexes (refer to Section\_12.2.31, “agent” for the details). However,\nthat creates redundant CPU and network load, and dist_threads\nis now strongly suggested instead.\nExample:\nlocal = chunk1\nlocal = chunk2"
    html_remove_elements:
        link: 'http://sphinxsearch.com/docs/current.html#conf-html-remove-elements'
        multi_value: false
        description: "A list of HTML elements for which to strip contents along with the elements themselves.\nOptional, default is empty string (do not strip contents of any elements).\n\nThis feature allows to strip element contents, ie. everything that\nis between the opening and the closing tags. It is useful to remove\nembedded scripts, CSS, etc. Short tag form for empty elements\n(ie. <br />) is properly supported; ie. the text that\nfollows such tag will not be removed.\n\nThe value is a comma-separated list of element (tag) names whose\ncontents should be removed. Tag names are case insensitive.\nExample:\nhtml_remove_elements = style, script"
    html_index_attrs:
        link: 'http://sphinxsearch.com/docs/current.html#conf-html-index-attrs'
        multi_value: false
        description: "A list of markup attributes to index when stripping HTML.\nOptional, default is empty (do not index markup attributes).\n\nSpecifies HTML markup attributes whose contents should be retained and indexed\neven though other HTML markup is stripped. The format is per-tag enumeration of\nindexable attributes, as shown in the example below.\nExample:\nhtml_index_attrs = img=alt,title; a=title;"
    html_strip:
        link: 'http://sphinxsearch.com/docs/current.html#conf-html-strip'
        multi_value: false
        description: "Whether to strip HTML markup from incoming full-text data.\nOptional, default is 0.\nKnown values are 0 (disable stripping) and 1 (enable stripping).\n\nBoth HTML tags and entities and considered markup and get processed.\nHTML tags are removed, their contents (i.e., everything between\n<P> and </P>) are left intact by default. You can choose\nto keep and index attributes of the tags (e.g., HREF attribute in\nan A tag, or ALT in an IMG one). Several well-known inline tags are\ncompletely removed, all other tags are treated as block level and\nreplaced with whitespace. For example, 'te<B>st</B>'\ntext will be indexed as a single keyword 'test', however,\n'te<P>st</P>' will be indexed as two keywords\n'te' and 'st'. Known inline tags are as follows: A, B, I, S, U, BASEFONT,\nBIG, EM, FONT, IMG, LABEL, SMALL, SPAN, STRIKE, STRONG, SUB, SUP, TT.\n\nHTML entities get decoded and replaced with corresponding UTF-8\ncharacters. Stripper supports both numeric forms (such as &#239;)\nand text forms (such as &oacute; or &nbsp;). All entities\nas specified by HTML4 standard are supported.\n\nStripping should work with\nproperly formed HTML and XHTML, but, just as most browsers, may produce\nunexpected results on malformed input (such as HTML with stray <'s\nor unclosed >'s).\n\nOnly the tags themselves, and also HTML comments, are stripped.\nTo strip the contents of the tags too (eg. to strip embedded scripts),\nsee html_remove_elements option.\nThere are no restrictions on tag names; ie. everything\nthat looks like a valid tag start, or end, or a comment\nwill be stripped.\nExample:\nhtml_strip = 1"
    phrase_boundary:
        link: 'http://sphinxsearch.com/docs/current.html#conf-phrase-boundary'
        multi_value: false
        description: "Phrase boundary characters list.\nOptional, default is empty.\n\nThis list controls what characters will be treated as phrase boundaries,\nin order to adjust word positions and enable phrase-level search\nemulation through proximity search. The syntax is similar\nto charset_table.\nMappings are not allowed and the boundary characters must not\noverlap with anything else.\n\nOn phrase boundary, additional word position increment (specified by\nphrase_boundary_step)\nwill be added to current word position. This enables phrase-level\nsearching through proximity queries: words in different phrases\nwill be guaranteed to be more than phrase_boundary_step distance\naway from each other; so proximity search within that distance\nwill be equivalent to phrase-level search.\n\nPhrase boundary condition will be raised if and only if such character\nis followed by a separator; this is to avoid abbreviations such as\nS.T.A.L.K.E.R or URLs being treated as several phrases.\nExample:\nphrase_boundary = ., ?, !, U+2026 # horizontal ellipsis"
    ngram_chars:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ngram-chars'
        multi_value: false
        description: "N-gram characters list.\nOptional, default is empty.\n\nTo be used in conjunction with in ngram_len,\nthis list defines characters, sequences of which are subject to N-gram extraction.\nWords comprised of other characters will not be affected by N-gram indexing\nfeature. The value format is identical to charset_table.\nExample:\nngram_chars = U+3000..U+2FA1F"
    ngram_len:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ngram-len'
        multi_value: false
        description: "N-gram lengths for N-gram indexing.\nOptional, default is 0 (disable n-gram indexing).\nKnown values are 0 and 1 (other lengths to be implemented).\n\nN-grams provide basic CJK (Chinese, Japanese, Korean) support for\nunsegmented texts. The issue with CJK searching is that there could be no\nclear separators between the words. Ideally, the texts would be filtered\nthrough a special program called segmenter that would insert separators\nin proper locations. However, segmenters are slow and error prone,\nand it's common to index contiguous groups of N characters, or n-grams,\ninstead.\n\nWhen this feature is enabled, streams of CJK characters are indexed\nas N-grams. For example, if incoming text is \"ABCDEF\" (where A to F represent\nsome CJK characters) and length is 1, in will be indexed as if\nit was \"A B C D E F\". (With length equal to 2, it would produce \"AB BC CD DE EF\";\nbut only 1 is supported at the moment.) Only those characters that are\nlisted in ngram_chars table\nwill be split this way; other ones will not be affected.\n\nNote that if search query is segmented, ie. there are separators between\nindividual words, then wrapping the words in quotes and using extended mode\nwill result in proper matches being found even if the text was not\nsegmented. For instance, assume that the original query is BC\_DEF.\nAfter wrapping in quotes on the application side, it should look\nlike \"BC\"\_\"DEF\" (with quotes). This query\nwill be passed to Sphinx and internally split into 1-grams too,\nresulting in \"B\_C\"\_\"D\_E\_F\" query, still with\nquotes that are the phrase matching operator. And it will match\nthe text even though there were no separators in the text.\n\nEven if the search query is not segmented, Sphinx should still produce\ngood results, thanks to phrase based ranking: it will pull closer phrase\nmatches (which in case of N-gram CJK words can mean closer multi-character\nword matches) to the top.\nExample:\nngram_len = 1"
    min_infix_len:
        link: 'http://sphinxsearch.com/docs/current.html#conf-min-infix-len'
        multi_value: false
        description: "Minimum infix prefix length to index.\nOptional, default is 0 (do not index infixes).\n\nInfix indexing allows to implement wildcard searching by 'start*', '*end', and '*middle*' wildcards.\nWhen minimum infix length is set to a positive number, indexer will index all the possible keyword infixes\n(ie. substrings) in addition to the keywords themselves. Too short infixes\n(below the minimum allowed length) will not be indexed. For instance,\nindexing a keyword \"test\" with min_infix_len=2 will result in indexing\n\"te\", \"es\", \"st\", \"tes\", \"est\" infixes along with the word itself.\nSearches against such index for \"es\" will match documents that contain\n\"test\" word, even if they do not contain \"es\" on itself. However,\nindexing infixes will make the index grow significantly (because of\nmany more indexed keywords), and will degrade both indexing and\nsearching times.\nThere's no automatic way to rank perfect word matches higher\nin an infix index, but the same tricks as with prefix indexes\ncan be applied.\nExample:\nmin_infix_len = 3"
    min_prefix_len:
        link: 'http://sphinxsearch.com/docs/current.html#conf-min-prefix-len'
        multi_value: false
        description: "Minimum word prefix length to index.\nOptional, default is 0 (do not index prefixes).\n\nPrefix indexing allows to implement wildcard searching by 'wordstart*' wildcards.\nWhen mininum prefix length is set to a positive number, indexer will index\nall the possible keyword prefixes (ie. word beginnings) in addition to the keywords\nthemselves. Too short prefixes (below the minimum allowed length) will not\nbe indexed.\n\nFor instance, indexing a keyword \"example\" with min_prefix_len=3\nwill result in indexing \"exa\", \"exam\", \"examp\", \"exampl\" prefixes along\nwith the word itself. Searches against such index for \"exam\" will match\ndocuments that contain \"example\" word, even if they do not contain \"exam\"\non itself. However, indexing prefixes will make the index grow significantly\n(because of many more indexed keywords), and will degrade both indexing\nand searching times.\n\nThere's no automatic way to rank perfect word matches higher\nin a prefix index, but there's a number of tricks to achieve that.\nFirst, you can setup two indexes, one with prefix indexing and one\nwithout it, search through both, and use SetIndexWeights()\ncall to combine weights. Second, you can rewriteyour extended-mode queries:\n\n$cl->Query ( \"( keyword | keyword* ) other keywords\" );\n\nExample:\nmin_prefix_len = 3"
    ignore_chars:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ignore-chars'
        multi_value: false
        description: "Ignored characters list.\nOptional, default is empty.\n\nUseful in the cases when some characters, such as soft hyphenation mark (U+00AD),\nshould be not just treated as separators but rather fully ignored.\nFor example, if '-' is simply not in the charset_table,\n\"abc-def\" text will be indexed as \"abc\" and \"def\" keywords.\nOn the contrary, if '-' is added to ignore_chars list, the same\ntext will be indexed as a single \"abcdef\" keyword.\n\nThe syntax is the same as for charset_table,\nbut it's only allowed to declare characters, and not allowed to map them. Also,\nthe ignored characters must not be present in charset_table.\nExample:\nignore_chars = U+AD"
    charset_table:
        link: 'http://sphinxsearch.com/docs/current.html#conf-charset-table'
        multi_value: false
        description: "Accepted characters table, with case folding rules.\nOptional, default value are latin and cyrillic characters.\n\ncharset_table is the main workhorse of Sphinx tokenizing process,\nie. the process of extracting keywords from document text or query text.\nIt controls what characters are accepted as valid and what are not,\nand how the accepted characters should be transformed (eg. should\nthe case be removed or not).\n\nYou can think of charset_table as of a big table that has a mapping\nfor each and every of 100K+ characters in Unicode. By default,\nevery character maps to 0, which means that it does not occur\nwithin keywords and should be treated as a separator. Once\nmentioned in the table, character is mapped to some other\ncharacter (most frequently, either to itself or to a lowercase\nletter), and is treated as a valid keyword part.\n\nThe expected value format is a commas-separated list of mappings.\nTwo simplest mappings simply declare a character as valid, and map\na single character to another single character, respectively.\nBut specifying the whole table in such form would result\nin bloated and barely manageable specifications. So there are\nseveral syntax shortcuts that let you map ranges of characters\nat once. The complete list is as follows:\nA->a\nSingle char mapping, declares source char 'A' as allowed\n        to occur within keywords and maps it to destination char 'a'\n        (but does not declare 'a' as allowed).\n    A..Z->a..z\nRange mapping, declares all chars in source range\n        as allowed and maps them to the destination range. Does not\n        declare destination range as allowed. Also checks ranges' lengths\n        (the lengths must be equal).\n    a\nStray char mapping, declares a character as allowed\n        and maps it to itself. Equivalent to a->a single char mapping.\n    a..z\nStray range mapping, declares all characters in range\n        as allowed and maps them to themselves. Equivalent to\n        a..z->a..z range mapping.\n    A..Z/2\nCheckerboard range map. Maps every pair of chars\n        to the second char. More formally, declares odd characters\n        in range as allowed and maps them to the even ones; also\n        declares even characters as allowed and maps them to themselves.\n        For instance, A..Z/2 is equivalent to A->B, B->B, C->D, D->D,\n        ..., Y->Z, Z->Z. This mapping shortcut is helpful for\n        a number of Unicode blocks where uppercase and lowercase\n        letters go in such interleaved order instead of contiguous\n        chunks.\n    \n\n\nControl characters with codes from 0 to 31 are always treated as separators.\nCharacters with codes 32 to 127, ie. 7-bit ASCII characters, can be used\nin the mappings as is. To avoid configuration file encoding issues,\n8-bit ASCII characters and Unicode characters must be specified in U+xxx form,\nwhere 'xxx' is hexadecimal codepoint number. This form can also be used\nfor 7-bit ASCII characters to encode special ones: eg. use U+20 to\nencode space, U+2E to encode dot, U+2C to encode comma.\n\nStarting with 2.2.3-beta, aliases \"english\" and \"russian\" are allowed at\ncontrol character mapping.\nExample:\n# default are English and Russian letters\ncharset_table = 0..9, A..Z->a..z, _, a..z, \\\n    U+410..U+42F->U+430..U+44F, U+430..U+44F, U+401->U+451, U+451\n\t\n# english charset defined with alias\ncharset_table = 0..9, english, _"
    min_word_len:
        link: 'http://sphinxsearch.com/docs/current.html#conf-min-word-len'
        multi_value: false
        description: "Minimum indexed word length.\nOptional, default is 1 (index everything).\n\nOnly those words that are not shorter than this minimum will be indexed.\nFor instance, if min_word_len is 4, then 'the' won't be indexed, but 'they' will be.\nExample:\nmin_word_len = 4"
    exceptions:
        link: 'http://sphinxsearch.com/docs/current.html#conf-exceptions'
        multi_value: false
        description: "Tokenizing exceptions file.\nOptional, default is empty.\n\nExceptions allow to map one or more tokens (including tokens with\ncharacters that would normally be excluded) to a single keyword.\nThey are similar to wordforms\nin that they also perform mapping, but have a number of important\ndifferences.\n\nStarting with version 2.1.1-beta small enough files are stored in the index\nheader, see Section\_12.2.13, “embedded_limit” for details.\n\nShort summary of the differences is as follows:\nexceptions are case sensitive, wordforms are not;\nexceptions can use special characters that are not in charset_table, wordforms fully obey charset_table;\nexceptions can underperform on huge dictionaries, wordforms handle millions of entries well.\n\n\n\nThe expected file format is also plain text, with one line per exception,\nand the line format is as follows:\n\nmap-from-tokens => map-to-token\n\nExample file:\n\nat & t => at&t\nAT&T => AT&T\nStandarten   Fuehrer => standartenfuhrer\nStandarten Fuhrer => standartenfuhrer\nMS Windows => ms windows\nMicrosoft Windows => ms windows\nC++ => cplusplus\nc++ => cplusplus\nC plus plus => cplusplus\n\nAll tokens here are case sensitive: they will not be processed by\ncharset_table rules. Thus, with\nthe example exceptions file above, \"at&t\" text will be tokenized as two\nkeywords \"at\" and \"t\", because of lowercase letters. On the other hand,\n\"AT&T\" will match exactly and produce single \"AT&T\" keyword.\n\nNote that this map-to keyword is a) always interpreted\nas a single word, and b) is both case and space\nsensitive! In our sample, \"ms windows\" query will not\nmatch the document with \"MS Windows\" text. The query will be interpreted\nas a query for two keywords, \"ms\" and \"windows\". And what \"MS Windows\"\ngets mapped to is a single keyword \"ms windows\",\nwith a space in the middle. On the other hand, \"standartenfuhrer\"\nwill retrieve documents with \"Standarten Fuhrer\" or \"Standarten Fuehrer\"\ncontents (capitalized exactly like this), or any capitalization variant\nof the keyword itself, eg. \"staNdarTenfUhreR\". (It won't catch\n\"standarten fuhrer\", however: this text does not match any of the\nlisted exceptions because of case sensitivity, and gets indexed\nas two separate keywords.)\n\nWhitespace in the map-from tokens list matters, but its amount does not.\nAny amount of the whitespace in the map-form list will match any other amount\nof whitespace in the indexed document or query. For instance, \"AT\_&\_T\"\nmap-from token will match \"AT\_\_\_\_&\_\_T\" text,\nwhatever the amount of space in both map-from part and the indexed text.\nSuch text will therefore be indexed as a special \"AT&T\" keyword,\nthanks to the very first entry from the sample.\n\nExceptions also allow to capture special characters (that are exceptions\nfrom general charset_table rules;\nhence the name). Assume that you generally do not want to treat '+'\nas a valid character, but still want to be able search for some exceptions\nfrom this rule such as 'C++'. The sample above will do just that, totally\nindependent of what characters are in the table and what are not.\n\nExceptions are applied to raw incoming document and query data\nduring indexing  and searching respectively. Therefore, to pick up\nchanges in the file it's required to reindex and restart\nsearchd.\nExample:\nexceptions = /usr/local/sphinx/data/exceptions.txt"
    embedded_limit:
        link: 'http://sphinxsearch.com/docs/current.html#conf-embedded-limit'
        multi_value: false
        description: "Embedded exceptions, wordforms, or stopwords file size limit.\nOptional, default is 16K.\nAdded in version 2.1.1-beta.\n\nBefore 2.1.1-beta, the contents of exceptions, wordforms, or stopwords\nfiles were always kept in the files. Only the file names were stored into\nthe index. Starting with 2.1.1-beta, indexer can either save the file name,\nor embed the file contents directly into the index. Files sized under\nembedded_limit get stored into the index. For bigger files,\nonly the file names are stored. This also simplifies moving index files\nto a different machine; you may get by just copying a single file.\n\nWith smaller files, such embedding reduces the number of the external\nfiles on which the index depends, and helps maintenance. But at the same\ntime it makes no sense to embed a 100 MB wordforms dictionary into a tiny\ndelta index. So there needs to be a size threshold, and embedded_limit\nis that threshold.\nExample:\nembedded_limit = 32K"
    wordforms:
        link: 'http://sphinxsearch.com/docs/current.html#conf-wordforms'
        multi_value: false
        description: "Word forms dictionary.\nOptional, default is empty.\n\nWord forms are applied after tokenizing the incoming text\nby charset_table rules.\nThey essentially let you replace one word with another. Normally,\nthat would be used to bring different word forms to a single\nnormal form (eg. to normalize all the variants such as \"walks\",\n\"walked\", \"walking\" to the normal form \"walk\"). It can also be used\nto implement stemming exceptions, because stemming is not applied\nto words found in the forms list.\n\nStarting with version 2.1.1-beta small enough files are stored in the index\nheader, see Section\_12.2.13, “embedded_limit” for details.\n\nDictionaries are used to normalize incoming words both during indexing\nand searching. Therefore, to pick up changes in wordforms file\nit's required to rotate index.\n\nWord forms support in Sphinx is designed to support big dictionaries well.\nThey moderately affect indexing speed: for instance, a dictionary with 1 million\nentries slows down indexing about 1.5 times. Searching speed is not affected at all.\nAdditional RAM impact is roughly equal to the dictionary file size,\nand dictionaries are shared across indexes: ie. if the very same 50 MB wordforms\nfile is specified for 10 different indexes, additional searchd\nRAM usage will be about 50 MB.\n\nDictionary file should be in a simple plain text format. Each line\nshould contain source and destination word forms, in UTF-8 encoding,\nseparated by \"greater\" sign. Rules from the\ncharset_table will be\napplied when the file is loaded. So basically it's as case sensitive\nas your other full-text indexed data, ie. typically case insensitive.\nHere's the file contents sample:\n\nwalks > walk\nwalked > walk\nwalking > walk\n\n\nThere is a bundled spelldump utility that\nhelps you create a dictionary file in the format Sphinx can read\nfrom source .dict and .aff\ndictionary files in ispell or MySpell\nformat (as bundled with OpenOffice).\n\nStarting with version 0.9.9-rc1, you can map several source words\nto a single destination word. Because the work happens on tokens,\nnot the source text, differences in whitespace and markup are ignored.\n\nStarting with version 2.1.1-beta, you can use \"=>\" instead of \">\". Comments\n(starting with \"#\" are also allowed. Finally, if a line starts with a tilde (\"~\")\nthe wordform will be applied after morphology, instead of before.\n\ncore 2 duo > c2d\ne6600 > c2d\ncore 2duo => c2d # Some people write '2duo' together...\n\n\nStating with version 2.2.4, you can specify multiple destination tokens:\n\ns02e02 > season 2 episode 2\ns3 e3 > season 3 episode 3\n\nExample:\nwordforms = /usr/local/sphinx/data/wordforms.txt\nwordforms = /usr/local/sphinx/data/alternateforms.txt\nwordforms = /usr/local/sphinx/private/dict*.txt\n\nStarting with version 2.1.1-beta you can specify several files and not\nonly just one. Masks can be used as a pattern, and all matching files will\nbe processed in simple ascending order. (If multi-byte codepages are used,\nand file names can include foreign characters, the resulting order may not\nbe exactly alphabetic.) If a same wordform definition is found in several\nfiles, the latter one is used, and it overrides previous definitions."
    stopwords:
        link: 'http://sphinxsearch.com/docs/current.html#conf-stopwords'
        multi_value: false
        description: "Stopword files list (space separated).\nOptional, default is empty.\n\nStopwords are the words that will not be indexed. Typically you'd\nput most frequent words in the stopwords list because they do not add\nmuch value to search results but consume a lot of resources to process.\n\nYou can specify several file names, separated by spaces. All the files\nwill be loaded. Stopwords file format is simple plain text. The encoding\nmust be UTF-8.\nFile data will be tokenized with respect to charset_table\nsettings, so you can use the same separators as in the indexed data.\n\nThe stemmers will normally be\napplied when parsing stopwords file. That might however lead to undesired\nresults. Starting with 2.1.1-beta, you can turn that off with\nstopwords_unstemmed.\n\nStarting with version 2.1.1-beta small enough files are stored in the index\nheader, see Section\_12.2.13, “embedded_limit” for details.\n\nWhile stopwords are not indexed, they still do affect the keyword positions.\nFor instance, assume that \"the\" is a stopword, that document 1 contains the line\n\"in office\", and that document 2 contains \"in the office\". Searching for \"in office\"\nas for exact phrase will only return the first document, as expected, even though\n\"the\" in the second one is stopped. That behavior can be tweaked through the\nstopword_step directive.\n\nStopwords files can either be created manually, or semi-automatically.\nindexer provides a mode that creates a frequency dictionary\nof the index, sorted by the keyword frequency, see --buildstops\nand --buildfreqs switch in Section\_7.1, “indexer command reference”.\nTop keywords from that dictionary can usually be used as stopwords.\nExample:\nstopwords = /usr/local/sphinx/data/stopwords.txt\nstopwords = stopwords-ru.txt stopwords-en.txt"
    min_stemming_len:
        link: 'http://sphinxsearch.com/docs/current.html#conf-min-stemming-len'
        multi_value: false
        description: "Minimum word length at which to enable stemming.\nOptional, default is 1 (stem everything).\nIntroduced in version 0.9.9-rc1.\n\nStemmers are not perfect, and might sometimes produce undesired results.\nFor instance, running \"gps\" keyword through Porter stemmer for English\nresults in \"gp\", which is not really the intent. min_stemming_len\nfeature lets you suppress stemming based on the source word length,\nie. to avoid stemming too short words. Keywords that are shorter than\nthe given threshold will not be stemmed. Note that keywords that are\nexactly as long as specified will be stemmed. So in order to avoid\nstemming 3-character keywords, you should specify 4 for the value.\nFor more finely grained control, refer to wordforms feature.\nExample:\nmin_stemming_len = 4"
    index_zones:
        link: 'http://sphinxsearch.com/docs/current.html#conf-index-zones'
        multi_value: false
        description: "A list of in-field HTML/XML zones to index.\nOptional, default is empty (do not index zones).\nIntroduced in version 2.0.1-beta.\n\nZones can be formally defined as follows. Everything between\nan opening and a matching closing tag is called a span, and\nthe aggregate of all spans corresponding sharing the same\ntag name is called a zone. For instance, everything between\nthe occurrences of <H1> and </H1> in the document\nfield belongs to H1 zone.\n\nZone indexing, enabled by index_zones directive,\nis an optional extension of the HTML stripper. So it will also\nrequire that the stripper\nis enabled (with html_strip = 1). The value of the\nindex_zones should be a comma-separated list of\nthose tag names and wildcards (ending with a star) that should\nbe indexed as zones.\n\nZones can nest and overlap arbitrarily. The only requirement\nis that every opening tag has a matching tag. You can also have\nan arbitrary number of both zones (as in unique zone names,\nsuch as H1) and spans (all the occurrences of those H1 tags)\nin a document.\nOnce indexed, zones can then be used for matching with\nthe ZONE operator, see Section\_5.3, “Extended query syntax”.\nExample:\nindex_zones = h*, th, title\n\nEarlier versions than 2.1.1-beta only provided this feature for plain\nindex files; currently, RT index files also provide it."
    index_sp:
        link: 'http://sphinxsearch.com/docs/current.html#conf-index-sp'
        multi_value: false
        description: "Whether to detect and index sentence and paragraph boundaries.\nOptional, default is 0 (do not detect and index).\nIntroduced in version 2.0.1-beta.\n\nThis directive enables sentence and paragraph boundary indexing.\nIt's required for the SENTENCE and PARAGRAPH operators to work.\nSentence boundary detection is based on plain text analysis, so you\nonly need to set index_sp = 1 to enable it. Paragraph\ndetection is however based on HTML markup, and happens in the\nHTML stripper.\nSo to index paragraph locations you also need to enable the stripper\nby specifying html_strip = 1. Both types of boundaries\nare detected based on a few built-in rules enumerated just below.\n\nSentence boundary detection rules are as follows.\nQuestion and exclamation signs (? and !) are always a sentence boundary.\nTrailing dot (.) is a sentence boundary, except:\n    When followed by a letter. That's considered a part of an abbreviation (as in \"S.T.A.L.K.E.R\" or \"Goldman Sachs S.p.A.\").\nWhen followed by a comma. That's considered an abbreviation followed by a comma (as in \"Telecom Italia S.p.A., founded in 1994\").\nWhen followed by a space and a small letter. That's considered an abbreviation within a sentence (as in \"News Corp. announced in February\").\nWhen preceded by a space and a capital letter, and followed by a space. That's considered a middle initial (as in \"John D. Doe\").\n\n\n\n\n\n\nParagraph boundaries are inserted at every block-level HTML tag.\nNamely, those are (as taken from HTML 4 standard) ADDRESS, BLOCKQUOTE,\nCAPTION, CENTER, DD, DIV, DL, DT, H1, H2, H3, H4, H5, LI, MENU, OL, P,\nPRE, TABLE, TBODY, TD, TFOOT, TH, THEAD, TR, and UL.\n\nBoth sentences and paragraphs increment the keyword position counter by 1.\nExample:\nindex_sp = 1"
    morphology:
        link: 'http://sphinxsearch.com/docs/current.html#conf-morphology'
        multi_value: false
        description: "A list of morphology preprocessors (stemmers or lemmatizers) to apply.\nOptional, default is empty (do not apply any preprocessor).\n\nMorphology preprocessors can be applied to the words being\nindexed to replace different forms of the same word with the base,\nnormalized form. For instance, English stemmer will normalize\nboth \"dogs\" and \"dog\" to \"dog\", making search results for\nboth searches the same.\n\nThere are 3 different morphology preprocessors that Sphinx implements:\nlemmatizers, stemmers, and phonetic algorithms.\nLemmatizer reduces a keyword form to a so-called lemma,\na proper normal form, or in other words, a valid natural language\nroot word. For example, \"running\" could be reduced to \"run\",\nthe infinitive verb form, and \"octopi\" would be reduced to \"octopus\",\nthe singular noun form. Note that sometimes a word form can have\nmultiple corresponding root words. For instance, by looking at\n\"dove\" it is not possible to tell whether this is a past tense\nof \"dive\" the verb as in \"He dove into a pool.\", or \"dove\" the noun\nas in \"White dove flew over the cuckoo's nest.\" In this case\nlemmatizer can generate all the possible root forms.\n\nStemmer reduces a keyword form to a so-called stem\nby removing and/or replacing certain well-known suffixes.\nThe resulting stem is however notguaranteed to be\na valid word on itself. For instance, with a Porter English\nstemmers \"running\" would still reduce to \"run\", which is fine,\nbut \"business\" would reduce to \"busi\", which is not a word,\nand \"octopi\" would not reduce at all. Stemmers are essentially\n(much) simpler but still pretty good replacements of full-blown\nlemmatizers.\n\nPhonetic algorithms replace the words with specially\ncrafted phonetic codes that are equal even when the words original\nare different, but phonetically close.\n\n\n\nThe morphology processors that come with our own built-in Sphinx\nimplementations are:\nEnglish, Russian, and German lemmatizers;\nEnglish, Russian, Arabic, and Czech stemmers;\nSoundEx and MetaPhone phonetic algorithms.\n\n\nYou can also link with libstemmer library for even more\nstemmers (see details below). With libstemmer, Sphinx also supports\nmorphological processing for more than 15 other languages. Binary\npackages should come prebuilt with libstemmer support, too.\n\nLemmatizer support was added in version 2.1.1-beta, starting with\na Russian lemmatizer. English and German lemmatizers were then added\nin version 2.2.1-beta.\n\nLemmatizers require a dictionary that needs to be\nadditionally downloaded from the Sphinx website. That dictionary\nneeds to be installed in a directory specified by\nlemmatizer_base\ndirective. Also, there is a\nlemmatizer_cache\ndirective that lets you speed up lemmatizing (and therefore\nindexing) by spending more RAM for, basically, an uncompressed\ncache of a dictionary.\n\nChinese segmentation using Rosette Linguistics Platform was added in 2.2.1-beta.\nIt is a much more precise but slower way (compared to n-grams) to segment Chinese documents.\ncharset_table must contain all Chinese characters except\nChinese punctuation marks because incoming documents are first processed by sphinx tokenizer and then the result\nis processed by RLP. Sphinx performs per-token language detection on the incoming documents. If token language is\nidentified as Chinese, it will only be processed the RLP, even if multiple morphology processors are specified.\nOtherwise, it will be processed by all the morphology processors specified in the \"morphology\" option. Rosette\nLinguistics Platform must be installed and configured and sphinx must be built with a --with-rlp switch. See also\nrlp_root,\nrlp_environment and\nrlp_context options.\nA batched version of RLP segmentation is also available (rlp_chinese_batched). It provides the\nsame functionality as the basic rlp_chinese segmentation, but enables batching documents before\nprocessing them by the RLP. Processing several documents at once can result in a substantial indexing speedup if\nthe documents are small (for example, less than 1k). See also\nrlp_max_batch_size and\nrlp_max_batch_docs options.\n\nAdditional stemmers provided by Snowball\nproject libstemmer library\ncan be enabled at compile time using --with-libstemmer configure option.\nBuilt-in English and Russian stemmers should be faster than their\nlibstemmer counterparts, but can produce slightly different results,\nbecause they are based on an older version.\n\nSoundex implementation matches that of MySQL. Metaphone implementation\nis based on Double Metaphone algorithm and indexes the primary code.\n\nBuilt-in values that are available for use in morphology\noption are as follows:\nnone - do not perform any morphology processing;\nlemmatize_ru - apply Russian lemmatizer and pick a single root form (added in 2.1.1-beta);\nlemmatize_en - apply English lemmatizer and pick a single root form (added in 2.2.1-beta);\nlemmatize_de - apply German lemmatizer and pick a single root form (added in 2.2.1-beta);\nlemmatize_ru_all - apply Russian lemmatizer and index all possible root forms (added in 2.1.1-beta);\nlemmatize_en_all - apply English lemmatizer and index all possible root forms (added in 2.2.1-beta);\nlemmatize_de_all - apply German lemmatizer and index all possible root forms (added in 2.2.1-beta);\nstem_en - apply Porter's English stemmer;\nstem_ru - apply Porter's Russian stemmer;\nstem_enru - apply Porter's English and Russian stemmers;\nstem_cz - apply Czech stemmer;\nstem_ar - apply Arabic stemmer (added in 2.1.1-beta);\nsoundex - replace keywords with their SOUNDEX code;\nmetaphone - replace keywords with their METAPHONE code.\nrlp_chinese - apply Chinese text segmentation using Rosette Linguistics Platform\nrlp_chinese_batched - apply Chinese text segmentation using Rosette Linguistics Platform with document batching\n\n\nAdditional values provided by libstemmer are in 'libstemmer_XXX' format,\nwhere XXX is libstemmer algorithm codename (refer to\nlibstemmer_c/libstemmer/modules.txt for a complete list).\n\nSeveral stemmers can be specified (comma-separated). They will be applied\nto incoming words in the order they are listed, and the processing will stop\nonce one of the stemmers actually modifies the word.\nAlso when wordforms feature is enabled\nthe word will be looked up in word forms dictionary first, and if there is\na matching entry in the dictionary, stemmers will not be applied at all.\nOr in other words, wordforms can be\nused to implement stemming exceptions.\nExample:\nmorphology = stem_en, libstemmer_sv"
    mlock:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mlock'
        multi_value: false
        description: "Memory locking for cached data.\nOptional, default is 0 (do not call mlock()).\n\nFor search performance, searchd preloads\na copy of .spa and .spi\nfiles in RAM, and keeps that copy in RAM at all times. But if there\nare no searches on the index for some time, there are no accesses\nto that cached copy, and OS might decide to swap it out to disk.\nFirst queries to such \"cooled down\" index will cause swap-in\nand their latency will suffer.\n\nSetting mlock option to 1 makes Sphinx lock physical RAM used\nfor that cached data using mlock(2) system call, and that prevents\nswapping (see man 2 mlock for details). mlock(2) is a privileged call,\nso it will require searchd to be either run\nfrom root account, or be granted enough privileges otherwise.\nIf mlock() fails, a warning is emitted, but index continues\nworking.\nExample:\nmlock = 1"
    path:
        link: 'http://sphinxsearch.com/docs/current.html#conf-path'
        multi_value: false
        description: "Index files path and file name (without extension).\nMandatory.\n\nPath specifies both directory and file name, but without extension.\nindexer will append different extensions\nto this path when generating final names for both permanent and\ntemporary index files. Permanent data files have several different\nextensions starting with '.sp'; temporary files' extensions\nstart with '.tmp'. It's safe to remove .tmp*\nfiles is if indexer fails to remove them automatically.\n\nFor reference, different index files store the following data:\n.spa stores document attributes (used in extern docinfo storage mode only);\n.spd stores matching document ID lists for each word ID;\n.sph stores index header information;\n.spi stores word lists (word IDs and pointers to .spd file);\n.spk stores kill-lists;\n.spm stores MVA data;\n.spp stores hit (aka posting, aka word occurrence) lists for each word ID;\n.sps stores string attribute data.\n.spe stores skip-lists to speed up doc-list filtering\n\n\nExample:\npath = /var/data/test1"
    source:
        link: 'http://sphinxsearch.com/docs/current.html#conf-source'
        multi_value: true
        description: "Adds document source to local index.\nMulti-value, mandatory.\n\nSpecifies document source to get documents from when the current\nindex is indexed. There must be at least one source. There may be multiple\nsources, without any restrictions on the source types: ie. you can pull\npart of the data from MySQL server, part from PostgreSQL, part from\nthe filesystem using xmlpipe2 wrapper.\n\nHowever, there are some restrictions on the source data. First,\ndocument IDs must be globally unique across all sources. If that\ncondition is not met, you might get unexpected search results.\nSecond, source schemas must be the same in order to be stored\nwithin the same index.\n\nNo source ID is stored automatically. Therefore, in order to be able\nto tell what source the matched document came from, you will need to\nstore some additional information yourself. Two typical approaches\ninclude:\nmangling document ID and encoding source ID in it:\n\nsource src1\n{\n    sql_query = SELECT id*10+1, ... FROM table1\n    ...\n}\n\nsource src2\n{\n    sql_query = SELECT id*10+2, ... FROM table2\n    ...\n}\n\n\n\nstoring source ID simply as an attribute:\n\nsource src1\n{\n    sql_query = SELECT id, 1 AS source_id FROM table1\n    sql_attr_uint = source_id\n    ...\n}\n\nsource src2\n{\n    sql_query = SELECT id, 2 AS source_id FROM table2\n    sql_attr_uint = source_id\n    ...\n}\n\n\n\n\nExample:\nsource = srcpart1\nsource = srcpart2\nsource = srcpart3"
    type:
        link: 'http://sphinxsearch.com/docs/current.html#conf-index-type'
        multi_value: false
        description: "Index type.\nKnown values are 'plain', 'distributed', 'rt' and 'template'.\nOptional, default is 'plain' (plain local index).\n\nSphinx supports several different types of indexes.\nVersions 0.9.x supported two index types: plain local indexes\nthat are stored and processed on the local machine; and distributed indexes,\nthat involve not only local searching but querying remote searchd\ninstances over the network as well (see Section\_5.8, “Distributed searching”).\nVersion 1.10-beta also adds support\nfor so-called real-time indexes (or RT indexes for short) that\nare also stored and processed locally, but additionally allow\nfor on-the-fly updates of the full-text index (see Chapter\_4, Real-time indexes).\nNote that attributes can be updated on-the-fly using\neither plain local indexes or RT ones.\nIn 2.2.1-beta template indexes was introduced. They are actually a\npseudo-indexes because they do not store any data. That means they do not create\nany files on your hard drive. But you can use them for keywords and snippets\ngeneration, which may be useful in some cases.\n\nIndex type setting lets you choose the needed type.\nBy default, plain local index type will be assumed.\nExample:\ntype = distributed"
    kbatch:
        link: https://sphinxsearch.com/docs/sphinx3.html#using-k-batches
        multi_value: false
        description: K-batches (“kill batches”) let you bulk delete older versions of the documents (rows) when bulk loading new data into Sphinx, for example, adding a new delta index on top of an older main archive index.
indexer:
    lemmatizer_cache:
        link: 'http://sphinxsearch.com/docs/current.html#conf-lemmatizer-cache'
        multi_value: false
        description: "Lemmatizer cache size.\nOptional, default is 256K.\nAdded in version 2.1.1-beta.\n\nOur lemmatizer implementation (see Section\_12.2.6, “morphology”\nfor a discussion of what lemmatizers are) uses a compressed dictionary\nformat that enables a space/speed tradeoff. It can either perform\nlemmatization off the compressed data, using more CPU but less RAM,\nor it can decompress and precache the dictionary either partially\nor fully, thus using less CPU but more RAM. And the lemmatizer_cache\ndirective lets you control how much RAM exactly can be spent for that\nuncompressed dictionary cache.\n\nCurrently, the only available dictionary is ru.pak, the Russian one.\nThe compressed dictionary is approximately 10 MB in size. Note that the\ndictionary stays in memory at all times, too. The default cache size\nis 256 KB. The accepted cache sizes are 0 to 2047 MB. It's safe to raise\nthe cache size too high; the lemmatizer will only use the needed memory.\nFor instance, the entire Russian dictionary decompresses to approximately\n110 MB; and thus setting lemmatizer_cache anywhere higher than that will\nnot affect the memory use: even when 1024 MB is allowed for the cache,\nif only 110 MB is needed, it will only use those 110 MB.\n\nOn our benchmarks, the total indexing time with different cache\nsizes was as follows:\n9.07 sec, morphology = lemmatize_ru, lemmatizer_cache = 0\n8.60 sec, morphology = lemmatize_ru, lemmatizer_cache = 256K\n8.33 sec, morphology = lemmatize_ru, lemmatizer_cache = 8M\n7.95 sec, morphology = lemmatize_ru, lemmatizer_cache = 128M\n6.85 sec, morphology = stem_ru (baseline)\n\n\nYour mileage may vary, but a simple rule of thumb would be to either\ngo with the small default 256 KB cache when pressed for memory, or spend\n128 MB extra RAM and cache the entire dictionary for maximum indexing\nperformance.\nExample:\nlemmatizer_cache = 256M # cache it all"
    on_file_field_error:
        link: 'http://sphinxsearch.com/docs/current.html#conf-on-file-field-error'
        multi_value: false
        description: "How to handle IO errors in file fields.\nOptional, default is ignore_field.\nIntroduced in version 2.0.2-beta.\n\nWhen there is a problem indexing a file referenced by a file field\n(Section\_12.1.27, “sql_file_field”), indexer can\neither index the document, assuming empty content in this particular field,\nor skip the document, or fail indexing entirely. on_file_field_error\ndirective controls that behavior. The values it takes are:\nignore_field, index the current document without field;\nskip_document, skip the current document but continue indexing;\nfail_index, fail indexing with an error message.\n\n\n\nThe problems that can arise are: open error, size error (file too big),\nand data read error. Warning messages on any problem will be given at all times,\nirregardless of the phase and the on_file_field_error setting.\n\nNote that with on_file_field_error = skip_document\ndocuments will only be ignored if problems are detected during\nan early check phase, and not during the actual file parsing\nphase. indexer will open every referenced file\nand check its size before doing any work, and then open it again\nwhen doing actual parsing work. So in case a file goes away\nbetween these two open attempts, the document will still be\nindexed.\nExample:\non_file_field_error = skip_document"
    max_file_field_buffer:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-file-field-buffer'
        multi_value: false
        description: "Maximum file field adaptive buffer size, bytes.\nOptional, default is 8 MB, minimum is 1 MB.\n\nFile field buffer is used to load files referred to from\nsql_file_field columns.\nThis buffer is adaptive, starting at 1 MB at first allocation,\nand growing in 2x steps until either file contents can be loaded,\nor maximum buffer size, specified by max_file_field_buffer\ndirective, is reached.\n\nThus, if there are no file fields are specified, no buffer\nis allocated at all.  If all files loaded during indexing are under\n(for example) 2 MB in size, but max_file_field_buffer\nvalue is 128 MB, peak buffer usage would still be only 2 MB. However,\nfiles over 128 MB would be entirely skipped.\nExample:\nmax_file_field_buffer = 128M"
    write_buffer:
        link: 'http://sphinxsearch.com/docs/current.html#conf-write-buffer'
        multi_value: false
        description: "Write buffer size, bytes.\nOptional, default is 1 MB.\n\nWrite buffers are used to write both temporary and final index\nfiles when indexing. Larger buffers reduce the number of required\ndisk writes. Memory for the buffers is allocated in addition to\nmem_limit. Note that several\n(currently up to 4) buffers for different files will be allocated,\nproportionally increasing the RAM usage.\nExample:\nwrite_buffer = 4M"
    max_xmlpipe2_field:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-xmlpipe2-field'
        multi_value: false
        description: "Maximum allowed field size for XMLpipe2 source type, bytes.\nOptional, default is 2 MB.\nExample:\nmax_xmlpipe2_field = 8M"
    max_iosize:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-iosize'
        multi_value: false
        description: "Maximum allowed I/O operation size, in bytes, for I/O throttling.\nOptional, default is 0 (unlimited).\n\nI/O throttling related option. It limits maximum file I/O operation\n(read or write) size for all operations performed by indexer.\nA value of 0 means that no limit is imposed.\nReads or writes that are bigger than the limit\nwill be split in several smaller operations, and counted as several operation\nby max_iops setting. At the time of this\nwriting, all I/O calls should be under 256 KB (default internal buffer size)\nanyway, so max_iosize values higher than 256 KB must not affect anything.\nExample:\nmax_iosize = 1048576"
    max_iops:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-iops'
        multi_value: false
        description: "Maximum I/O operations per second, for I/O throttling.\nOptional, default is 0 (unlimited).\n\nI/O throttling related option.\nIt limits maximum count of I/O operations (reads or writes) per any given second.\nA value of 0 means that no limit is imposed.\n\nindexer can cause bursts of intensive disk I/O during\nindexing, and it might desired to limit its disk activity  (and keep something\nfor other programs running on the same machine, such as searchd).\nI/O throttling helps to do that. It works by enforcing a minimum guaranteed\ndelay between subsequent disk I/O operations performed by indexer.\nModern SATA HDDs are able to perform up to 70-100+ I/O operations per second\n(that's mostly limited by disk heads seek time). Limiting indexing I/O\nto a fraction of that can help reduce search performance degradation\ncaused by indexing.\nExample:\nmax_iops = 40"
    mem_limit:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mem-limit'
        multi_value: false
        description: "Indexing RAM usage limit.\nOptional, default is 128M.\n\nEnforced memory usage limit that the indexer\nwill not go above. Can be specified in bytes, or kilobytes\n(using K postfix), or megabytes (using M postfix); see the example.\nThis limit will be automatically raised if set to extremely low value\ncausing I/O buffers to be less than 8 KB; the exact lower bound\nfor that depends on the indexed data size. If the buffers are\nless than 256 KB, a warning will be produced.\n\nMaximum possible limit is 2047M. Too low values can hurt\nindexing speed, but 256M to 1024M should be enough for most\nif not all datasets. Setting this value too high can cause\nSQL server timeouts. During the document collection phase,\nthere will be periods when the memory buffer is partially\nsorted and no communication with the database is performed;\nand the database server can timeout. You can resolve that\neither by raising timeouts on SQL server side or by lowering\nmem_limit.\nExample:\nmem_limit = 256M\n# mem_limit = 262144K # same, but in KB\n# mem_limit = 268435456 # same, but in bytes"
searchd:
    agent_retry_delay:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-retry-delay'
        multi_value: false
        description: "Integer, in milliseconds. Specifies the delay sphinx rest before retrying to query a remote agent in case it fails.\nThe value has sense only if non-zero agent_retry_count \nor non-zero per-query OPTION retry_count specified. Default is 500. This value may be also specified \non per-query basis using 'OPTION retry_delay=XXX' clause. If per-query option exists, it will override the one specified in config."
    agent_retry_count:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-retry-count'
        multi_value: false
        description: "Integer, specifies how many times sphinx will try to connect and query remote agents in distributed index before reporting \nfatal query error. Default is 0 (i.e. no retries). This value may be also specified on per-query basis using \n'OPTION retry_count=XXX' clause. If per-query option exists, it will override the one specified in config."
    agent_query_timeout:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-query-timeout-default'
        multi_value: false
        description: "Instance-wide defaults for agent_query_timeout parameter.\nThe last defined in distributed (network) indexes, or also may be overrided per-query using OPTION clause."
    agent_connect_timeout:
        link: 'http://sphinxsearch.com/docs/current.html#conf-agent-connect-timeout-default'
        multi_value: false
        description: "Instance-wide defaults for agent_connect_timeout parameter.\nThe last defined in distributed (network) indexes."
    query_log_min_msec:
        link: 'http://sphinxsearch.com/docs/current.html#conf-query-log-min-msec'
        multi_value: false
        description: "Limit (in milliseconds) that prevents the query from being written to the query log.\nOptional, default is 0 (all queries are written to the query log). This directive\nspecifies that only queries with execution times that exceed the specified limit will be logged."
    ondisk_attrs_default:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ondisk-attrs-default'
        multi_value: false
        description: "Instance-wide defaults for ondisk_attrs\ndirective. Optional, default is 0 (all attributes are loaded in memory). This\ndirective lets you specify the default value of ondisk_attrs for all indexes\nserved by this copy of searchd. Per-index directives take precedence, and will\noverwrite this instance-wide default value, allowing for fine-grain control."
    shutdown_timeout:
        link: 'http://sphinxsearch.com/docs/current.html#conf-shutdown-timeout'
        multi_value: false
        description: "searchd --stopwait wait time, in seconds.\nOptional, default is 3 seconds.\nAdded in 2.2.1-beta.\n\nWhen you run searchd --stopwait your daemon needs to perform some\nactivities before stopping like finishing queries, flushing RT RAM chunk,\nflushing attributes and updating binlog. And it requires some time.\nsearchd --stopwait will wait up to shutdown_time seconds for daemon to\nfinish its jobs. Suitable time depends on your index size and load.\nExample:\nshutdown_timeout = 5 # wait for up to 5 seconds"
    predicted_time_costs:
        link: 'http://sphinxsearch.com/docs/current.html#conf-predicted-time-costs'
        multi_value: false
        description: "Costs for the query time prediction model, in nanoseconds.\nOptional, default is \"doc=64, hit=48, skip=2048, match=64\" (without the quotes).\nAdded in 2.1.1-beta.\n\nTerminating queries before completion based on their execution time\n(via either SetMaxQueryTime()\nAPI call, or SELECT ... OPTION max_query_time\nSphinxQL statement) is a nice safety net, but it comes with an inborn drawback:\nindeterministic (unstable) results. That is, if you repeat the very same (complex)\nsearch query with a time limit several times, the time limit will get hit\nat different stages, and you will get different result sets.\n\nStarting with 2.1.1-beta, there is a new option,\nSELECT ... OPTION max_predicted_time,\nthat lets you limit the query time and get stable,\nrepeatable results. Instead of regularly checking the actual current time\nwhile evaluating the query, which is indeterministic, it predicts the current\nrunning time using a simple linear model instead:\n\npredicted_time =\n    doc_cost * processed_documents +\n    hit_cost * processed_hits +\n    skip_cost * skiplist_jumps +\n    match_cost * found_matches\n\nThe query is then terminated early when the predicted_time\nreaches a given limit.\n\nOf course, this is not a hard limit on the actual time spent (it is, however,\na hard limit on the amount of processing work done), and\na simple linear model is in no way an ideally precise one. So the wall clock time\nmay be either below or over the target limit. However,\nthe error margins are quite acceptable: for instance, in our experiments with\na 100 msec target limit the majority of the test queries fell into a 95 to 105 msec\nrange, and all of the queries were in a 80 to 120 msec range.\nAlso, as a nice side effect, using the modeled query time instead of measuring\nactual run time results in somewhat less gettimeofday() calls, too.\n\nNo two server makes and models are identical, so predicted_time_costs\ndirective lets you configure the costs for the model above. For convenience, they are\nintegers, counted in nanoseconds. (The limit in max_predicted_time is counted\nin milliseconds, and having to specify cost values as 0.000128 ms instead of 128 ns\nis somewhat more error prone.) It is not necessary to specify all 4 costs at once,\nas the missed one will take the default values. However, we strongly suggest\nto specify all of them, for readability.\nExample:\npredicted_time_costs = doc=128, hit=96, skip=4096, match=128"
    rt_merge_maxiosize:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-merge-maxiosize'
        multi_value: false
        description: "A maximum size of an I/O operation that the RT chunks merge\nthread is allowed to start.\nOptional, default is 0 (no limit).\nAdded in 2.1.1-beta.\n\nThis directive lets you throttle down the I/O impact arising from\nthe OPTIMIZE statements. I/Os bigger than this limit will be\nbroken down into 2 or more I/Os, which will then be accounted as separate I/Os\nwith regards to the rt_merge_iops\nlimit. Thus, it is guaranteed that all the optimization activity will not\ngenerate more than (rt_merge_iops * rt_merge_maxiosize) bytes of disk I/O\nper second.\nExample:\nrt_merge_maxiosize = 1M"
    rt_merge_iops:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-merge-iops'
        multi_value: false
        description: "A maximum number of I/O operations (per second) that the RT chunks merge thread is allowed to start.\nOptional, default is 0 (no limit). Added in 2.1.1-beta.\n\nThis directive lets you throttle down the I/O impact arising from\nthe OPTIMIZE statements. It is guaranteed that all the\nRT optimization activity will not generate more disk iops (I/Os per second)\nthan the configured limit. Modern SATA drives can perform up to around 100 I/O operations per\nsecond, and limiting rt_merge_iops can reduce search performance degradation caused by merging.\nExample:\nrt_merge_iops = 40"
    persistent_connections_limit:
        link: 'http://sphinxsearch.com/docs/current.html#conf-persistent-connections-limit'
        multi_value: false
        description: "The maximum # of simultaneous persistent connections to remote persistent agents.\nEach time connecting agent defined under 'agent_persistent' we try to reuse existing connection (if any), or connect and save the connection for the future.\nHowever we can't hold unlimited # of such persistent connections, since each one holds a worker on agent size (and finally we'll receive the 'maxed out' error,\nwhen all of them are busy). This very directive limits the number. It affects the num of connections to each agent's host, across all distributed indexes.\n\nIt is reasonable to set the value equal or less than max_children option of the agents.\nExample:\npersistent_connections_limit = 29 # assume that each host of agents has max_children = 30 (or 29)."
    ha_period_karma:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ha-period-karma'
        multi_value: false
        description: "Agent mirror statistics window size, in seconds.\nOptional, default is 60.\nAdded in 2.1.1-beta.\n\nFor a distributed index with agent mirrors in it (see more in ???),\nmaster tracks several different per-mirror counters. These counters\nare then used for failover and balancing. (Master picks the best\nmirror to use based on the counters.) Counters are accumulated in\nblocks of ha_period_karma seconds.\n\nAfter beginning a new block, master may still use the accumulated\nvalues from the previous one, until the new one is half full. Thus,\nany previous history stops affecting the mirror choice after\n1.5 times ha_period_karma seconds at most.\n\nDespite that at most 2 blocks are used for mirror selection,\nupto 15 last blocks are actually stored, for instrumentation purposes.\nThey can be inspected using\nSHOW AGENT STATUS\nstatement.\nExample:\nha_period_karma = 120"
    ha_ping_interval:
        link: 'http://sphinxsearch.com/docs/current.html#conf-ha-ping-interval'
        multi_value: false
        description: "Interval between agent mirror pings, in milliseconds.\nOptional, default is 1000.\nAdded in 2.1.1-beta.\n\nFor a distributed index with agent mirrors in it (see more in ???),\nmaster sends all mirrors a ping command during the idle periods.\nThis is to track the current agent status (alive or dead, network\nroundtrip, etc). The interval between such pings is defined\nby this directive.\n\nTo disable pings, set ha_ping_interval to 0.\nExample:\nha_ping_interval = 0"
    sphinxql_state:
        link: 'http://sphinxsearch.com/docs/current.html#conf-sphinxql-state'
        multi_value: false
        description: "Path to a file where current SphinxQL state will be serialized.\nAvailable since version 2.1.1-beta.\n\nOn daemon startup, this file gets replayed. On eligible state changes (eg. SET GLOBAL),\nthis file gets rewritten automatically. This can prevent a hard-to-diagnose problem:\nIf you load UDF functions, but Sphinx crashes, when it\ngets (automatically) restarted, your UDF and global variables will no longer be available;\nusing persistent state helps a graceful recovery with no such surprises.\nExample:\nsphinxql_state = uservars.sql"
    prefork_rotation_throttle:
        link: 'http://sphinxsearch.com/docs/current.html#conf-prefork-rotation-throttle'
        multi_value: false
        description: "Delay between restarting preforked children on index rotation, in milliseconds.\nOptional, default is 0 (no delay).\nIntroduced in version 2.0.2-beta.\n\nWhen running in workers = prefork\nmode, every index rotation needs to restart all children to propagate the newly\nloaded index data changes. Restarting all of them at once might put excessive\nstrain on CPU and/or network connections. (For instance, when the application\nkeeps a bunch of open persistent connections to different children, and all those\nchildren restart.) Those bursts can be throttled down with\nprefork_rotation_throttle directive. Note that\nthe children will be restarted sequentially, and thus \"old\" results might\npersist for a few more seconds. For instance, if\nprefork_rotation_throttle is set to 50 (milliseconds), and\nthere are 30 children, then the last one would only be actually\nrestarted 1.5 seconds (50*30=1500 milliseconds) after\nthe \"rotation finished\" message in the searchd event log.\nExample:\nprefork_rotation_throttle = 50 # throttle children restarts by 50 msec each"
    watchdog:
        link: 'http://sphinxsearch.com/docs/current.html#conf-watchdog'
        multi_value: false
        description: "Threaded server watchdog.\nOptional, default is 1 (watchdog enabled).\nIntroduced in version 2.0.1-beta.\n\nA crashed query in threads multi-processing mode\n(workers = threads)\ncan take down the entire server. With watchdog feature enabled,\nsearchd additionally keeps a separate lightweight\nprocess that monitors the main server process, and automatically\nrestarts the latter in case of abnormal termination. Watchdog\nis enabled by default.\nExample:\nwatchdog = 0 # disable watchdog"
    expansion_limit:
        link: 'http://sphinxsearch.com/docs/current.html#conf-expansion-limit'
        multi_value: false
        description: "The maximum number of expanded keywords for a single wildcard.\nOptional, default is 0 (no limit).\nIntroduced in version 2.0.1-beta.\n\nWhen doing substring searches against indexes built with\ndict = keywords enabled, a single wildcard may\npotentially result in thousands and even millions of matched\nkeywords (think of matching 'a*' against the entire Oxford\ndictionary). This directive lets you limit the impact\nof such expansions. Setting expansion_limit = N\nrestricts expansions to no more than N of the most frequent\nmatching keywords (per each wildcard in the query).\nExample:\nexpansion_limit = 16"
    thread_stack:
        link: 'http://sphinxsearch.com/docs/current.html#conf-thread-stack'
        multi_value: false
        description: "Per-thread stack size.\nOptional, default is 1M.\nIntroduced in version 2.0.1-beta.\n\nIn the workers = threads mode, every request is processed\nwith a separate thread that needs its own stack space. By default, 1M per\nthread are allocated for stack. However, extremely complex search requests\nmight eventually exhaust the default stack and require more. For instance,\na query that matches a thousands of keywords (either directly or through\nterm expansion) can eventually run out of stack. Previously, that resulted\nin crashes. Starting with 2.0.1-beta, searchd attempts\nto estimate the expected stack use, and blocks the potentially dangerous\nqueries. To process such queries, you can either the thread stack size\nby using the thread_stack directive (or switch to a different\nworkers setting if that is possible).\n\nA query with N levels of nesting is estimated to require approximately\n30+0.16*N KB of stack, meaning that the default 64K is enough for queries\nwith upto 250 levels, 150K for upto 700 levels, etc. If the stack size limit\nis not met, searchd fails the query and reports\nthe required stack size in the error message.\nExample:\nthread_stack = 256K"
    rt_flush_period:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rt-flush-period'
        multi_value: false
        description: "RT indexes RAM chunk flush check period, in seconds.\nOptional, default is 10 hours.\nIntroduced in version 2.0.1-beta.\n\nActively updated RT indexes that however fully fit in RAM chunks\ncan result in ever-growing binlogs, impacting disk use and crash\nrecovery time. With this directive the search daemon performs\nperiodic flush checks, and eligible RAM chunks can get saved,\nenabling consequential binlog cleanup. See Section\_4.4, “Binary logging”\nfor more details.\nExample:\nrt_flush_period = 3600 # 1 hour"
    mysql_version_string:
        link: 'http://sphinxsearch.com/docs/current.html#conf-mysql-version-string'
        multi_value: false
        description: "A server version string to return via MySQL protocol.\nOptional, default is empty (return Sphinx version).\nIntroduced in version 2.0.1-beta.\n\nSeveral picky MySQL client libraries depend on a particular version\nnumber format used by MySQL, and moreover, sometimes choose a different\nexecution path based on the reported version number (rather than the\nindicated capabilities flags). For instance, Python MySQLdb 1.2.2 throws\nan exception when the version number is not in X.Y.ZZ format; MySQL .NET\nconnector 6.3.x fails internally on version numbers 1.x along with\na certain combination of flags, etc. To workaround that, you can use\nmysql_version_string directive and have searchd\nreport a different version to clients connecting over MySQL protocol.\n(By default, it reports its own version.)\nExample:\nmysql_version_string = 5.0.37"
    collation_libc_locale:
        link: 'http://sphinxsearch.com/docs/current.html#conf-collation-libc-locale'
        multi_value: false
        description: "Server libc locale.\nOptional, default is C.\nIntroduced in version 2.0.1-beta.\n\nSpecifies the libc locale, affecting the libc-based collations.\nRefer to Section\_5.12, “Collations” section for the details.\nExample:\ncollation_libc_locale = fr_FR"
    collation_server:
        link: 'http://sphinxsearch.com/docs/current.html#conf-collation-server'
        multi_value: false
        description: "Default server collation.\nOptional, default is libc_ci.\nIntroduced in version 2.0.1-beta.\n\nSpecifies the default collation used for incoming requests.\nThe collation can be overridden on a per-query basis.\nRefer to Section\_5.12, “Collations” section for the list of available collations and other details.\nExample:\ncollation_server = utf8_ci"
    snippets_file_prefix:
        link: 'http://sphinxsearch.com/docs/current.html#conf-snippets-file-prefix'
        multi_value: false
        description: "A prefix to prepend to the local file names when generating snippets.\nOptional, default is empty.\nIntroduced in version 2.1.1-beta.\n\nThis prefix can be used in distributed snippets generation along with\nload_files or load_files_scattered options.\n\nNote how this is a prefix, and not a path! Meaning that if a prefix\nis set to \"server1\" and the request refers to \"file23\", searchd\nwill attempt to open \"server1file23\" (all of that without quotes). So if you\nneed it to be a path, you have to mention the trailing slash.\n\nNote also that this is a local option, it does not affect the agents anyhow.\nSo you can safely set a prefix on a master server. The requests routed to the\nagents will not be affected by the master's setting. They will however be affected\nby the agent's own settings.\n\nThis might be useful, for instance, when the document storage locations\n(be those local storage or NAS mountpoints) are inconsistent across the servers.\nExample:\nsnippets_file_prefix = /mnt/common/server1/"
    binlog_max_log_size:
        link: 'http://sphinxsearch.com/docs/current.html#conf-binlog-max-log-size'
        multi_value: false
        description: "Maximum binary log file size.\nOptional, default is 0 (do not reopen binlog file based on size).\nIntroduced in version 1.10-beta.\n\nA new binlog file will be forcibly opened once the current binlog file\nreaches this limit. This achieves a finer granularity of logs and can yield\nmore efficient binlog disk usage under certain borderline workloads.\nExample:\nbinlog_max_log_size = 16M"
    binlog_flush:
        link: 'http://sphinxsearch.com/docs/current.html#conf-binlog-flush'
        multi_value: false
        description: "Binary log transaction flush/sync mode.\nOptional, default is 2 (flush every transaction, sync every second).\nIntroduced in version 1.10-beta.\n\nThis directive controls how frequently will binary log be flushed\nto OS and synced to disk. Three modes are supported:\n0, flush and sync every second. Best performance,\nbut up to 1 second worth of committed transactions can be lost\nboth on daemon crash, or OS/hardware crash.\n\n1, flush and sync every transaction. Worst performance,\nbut every committed transaction data is guaranteed to be saved.\n\n2, flush every transaction, sync every second.\nGood performance, and every committed transaction is guaranteed\nto be saved in case of daemon crash. However, in case of OS/hardware\ncrash up to 1 second worth of committed transactions can be lost.\n\n\n\n\nFor those familiar with MySQL and InnoDB, this directive is entirely\nsimilar to innodb_flush_log_at_trx_commit. In most\ncases, the default hybrid mode 2 provides a nice balance of speed\nand safety, with full RT index data protection against daemon crashes,\nand some protection against hardware ones.\nExample:\nbinlog_flush = 1 # ultimate safety, low speed"
    binlog_path:
        link: 'http://sphinxsearch.com/docs/current.html#conf-binlog-path'
        multi_value: false
        description: "Binary log (aka transaction log) files path.\nOptional, default is build-time configured data directory.\nIntroduced in version 1.10-beta.\n\nBinary logs are used for crash recovery of RT index data, and also of\nattributes updates of plain disk indices that\nwould otherwise only be stored in RAM until flush.  When logging is enabled,\nevery transaction COMMIT-ted into RT index gets written into\na log file.  Logs are then automatically replayed on startup\nafter an unclean shutdown, recovering the logged changes.\n\nbinlog_path directive specifies the binary log\nfiles location.  It should contain just the path; searchd\nwill create and unlink multiple binlog.* files in that path as necessary\n(binlog data, metadata, and lock files, etc).\n\nEmpty value disables binary logging. That improves performance,\nbut puts RT index data at risk.\n\nWARNING! It is strongly recommended to always explicitly define 'binlog_path' option in your config.\nOtherwise, the default path, which in most cases is the same as working folder, may point to the\nfolder with no write access (for example, /usr/local/var/data). In this case, the searchd\nwill not start at all.\nExample:\nbinlog_path = # disable logging\nbinlog_path = /var/data # /var/data/binlog.001 etc will be created"
    dist_threads:
        link: 'http://sphinxsearch.com/docs/current.html#conf-dist-threads'
        multi_value: false
        description: "Max local worker threads to use for parallelizable requests (searching a distributed index; building a batch of snippets).\nOptional, default is 0, which means to disable in-request parallelism.\nIntroduced in version 1.10-beta.\n\nDistributed index can include several local indexes. dist_threads\nlets you easily utilize multiple CPUs/cores for that (previously existing\nalternative was to specify the indexes as remote agents, pointing searchd\nto itself and paying some network overheads).\n\nWhen set to a value N greater than 1, this directive will create up to\nN threads for every query, and schedule the specific searches within these\nthreads. For example, if there are 7 local indexes to search and dist_threads\nis set to 2, then 2 parallel threads would be created: one that sequentially\nsearches 4 indexes, and another one that searches the other 3 indexes.\n\nIn case of CPU bound workload, setting dist_threads\nto 1x the number of cores is advised (creating more threads than cores\nwill not improve query time). In case of mixed CPU/disk bound workload\nit might sometimes make sense to use more (so that all cores could be\nutilizes even when there are threads that wait for I/O completion).\n\nNote that dist_threads does not require\nthreads MPM. You can perfectly use it with fork or prefork MPMs too.\n\nStarting with version 2.0.1-beta, building a batch of snippets\nwith load_files flag enabled can also be parallelized.\nUp to dist_threads threads are be created to process\nthose files. That speeds up snippet extraction when the total amount\nof document data to process is significant (hundreds of megabytes).\nExample:\nindex dist_test\n{\n    type = distributed\n    local = chunk1\n    local = chunk2\n    local = chunk3\n    local = chunk4\n}\n\n# ...\n\ndist_threads = 4"
    workers:
        link: 'http://sphinxsearch.com/docs/current.html#conf-workers'
        multi_value: false
        description: "Multi-processing mode (MPM).\nOptional; allowed values are none, fork, prefork, and threads.\nDefault is threads.\nIntroduced in version 1.10-beta.\n\nLets you choose how searchd processes multiple\nconcurrent requests. The possible values are:\nnone\nAll requests will be handled serially, one-by-one.\n        Prior to 1.x, this was the only mode available on Windows.\n    fork\nA new child process will be forked to handle every\n        incoming request.\n    prefork\nOn startup, searchd will pre-fork\n        a number of worker processes, and pass the incoming requests\n        to one of those children.\n    threads\nA new thread will be created to handle every\n        incoming request. This is the only mode compatible with\n        RT indexing backend. This is a default value.\n    \n\n\nHistorically, searchd used fork-based model,\nwhich generally performs OK but spends a noticeable amount of CPU\nin fork() system call when there's a high amount of (tiny) requests\nper second. Prefork mode was implemented to alleviate that; with\nprefork, worker processes are basically only created on startup\nand re-created on index rotation, somewhat reducing fork() call\npressure.\n\nThreads mode was implemented along with RT backend and is required\nto use RT indexes. (Regular disk-based indexes work in all the\navailable modes.)\nExample:\nworkers = threads"
    max_batch_queries:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-batch-queries'
        multi_value: false
        description: "Limits the amount of queries per batch.\nOptional, default is 32.\n\nMakes searchd perform a sanity check of the amount of the queries\nsubmitted in a single batch when using multi-queries.\nSet it to 0 to skip the check.\nExample:\nmax_batch_queries = 256"
    read_unhinted:
        link: 'http://sphinxsearch.com/docs/current.html#conf-read-unhinted'
        multi_value: false
        description: "Unhinted read size.\nOptional, default is 32K.\n\nWhen querying, some reads know in advance exactly how much data\nis there to be read, but some currently do not. Most prominently,\nhit list size in not currently known in advance. This setting\nlest you control how much data to read in such cases. It will\nimpact hit list IO time, reducing it for lists larger than\nunhinted read size, but raising it for smaller lists. It will\nnot affect RAM use because read buffer will be already\nallocated. So it should be not greater than read_buffer.\nExample:\nread_unhinted = 32K"
    read_buffer:
        link: 'http://sphinxsearch.com/docs/current.html#conf-read-buffer'
        multi_value: false
        description: "Per-keyword read buffer size.\nOptional, default is 256K.\n\nFor every keyword occurrence in every search query, there are\ntwo associated read buffers (one for document list and one for\nhit list). This setting lets you control their sizes, increasing\nper-query RAM use, but possibly decreasing IO time.\nExample:\nread_buffer = 1M"
    listen_backlog:
        link: 'http://sphinxsearch.com/docs/current.html#conf-listen-backlog'
        multi_value: false
        description: "TCP listen backlog.\nOptional, default is 5.\n\nWindows builds currently (as of 0.9.9) can only process the requests\none by one. Concurrent requests will be enqueued by the TCP stack\non OS level, and requests that can not be enqueued will immediately\nfail with \"connection refused\" message. listen_backlog directive\ncontrols the length of the connection queue. Non-Windows builds\nshould work fine with the default value.\nExample:\nlisten_backlog = 20"
    max_filter_values:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-filter-values'
        multi_value: false
        description: "Maximum allowed per-filter values count.\nOnly used for internal sanity checks, does not directly affect RAM use or performance.\nOptional, default is 4096.\nIntroduced in version 0.9.9-rc1.\nExample:\nmax_filter_values = 16384"
    max_filters:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-filters'
        multi_value: false
        description: "Maximum allowed per-query filter count.\nOnly used for internal sanity checks, does not directly affect RAM use or performance.\nOptional, default is 256.\nIntroduced in version 0.9.9-rc1.\nExample:\nmax_filters = 1024"
    max_packet_size:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-packet-size'
        multi_value: false
        description: "Maximum allowed network packet size.\nLimits both query packets from clients, and response packets from remote agents in distributed environment.\nOnly used for internal sanity checks, does not directly affect RAM use or performance.\nOptional, default is 8M.\nIntroduced in version 0.9.9-rc1.\nExample:\nmax_packet_size = 32M"
    unlink_old:
        link: 'http://sphinxsearch.com/docs/current.html#conf-unlink-old'
        multi_value: false
        description: "Whether to unlink .old index copies on successful rotation.\nOptional, default is 1 (do unlink).\nExample:\nunlink_old = 0"
    preopen_indexes:
        link: 'http://sphinxsearch.com/docs/current.html#conf-preopen-indexes'
        multi_value: false
        description: "Whether to forcibly preopen all indexes on startup.\nOptional, default is 1 (preopen everything).\n\nStarting with 2.0.1-beta, the default value for this\noption is now 1 (foribly preopen all indexes). In prior\nversions, it used to be 0 (use per-index settings).\n\nWhen set to 1, this directive overrides and enforces\npreopen on all indexes.\nThey will be preopened, no matter what is the per-index\npreopen setting. When set to 0, per-index\nsettings can take effect. (And they default to 0.)\n\nPre-opened indexes avoid races between search queries\nand rotations that can cause queries to fail occasionally.\nThey also make searchd use more file\nhandles. In most scenarios it's therefore preferred and\nrecommended to preopen indexes.\nExample:\npreopen_indexes = 1"
    seamless_rotate:
        link: 'http://sphinxsearch.com/docs/current.html#conf-seamless-rotate'
        multi_value: false
        description: "Prevents searchd stalls while rotating indexes with huge amounts of data to precache.\nOptional, default is 1 (enable seamless rotation). On Windows systems seamless rotation is disabled by default.\n\nIndexes may contain some data that needs to be precached in RAM.\nAt the moment, .spa, .spi and\n.spm files are fully precached (they contain attribute data,\nMVA data, and keyword index, respectively.)\nWithout seamless rotate, rotating an index tries to use as little RAM\nas possible and works as follows:\nnew queries are temporarily rejected (with \"retry\" error code);\nsearchd waits for all currently running queries to finish;\nold index is deallocated and its files are renamed;\nnew index files are renamed and required RAM is allocated;\nnew index attribute and dictionary data is preloaded to RAM;\nsearchd resumes serving queries from new index.\n\n\n\nHowever, if there's a lot of attribute or dictionary data, then preloading step\ncould take noticeable time - up to several minutes in case of preloading 1-5+ GB files.\n\nWith seamless rotate enabled, rotation works as follows:\nnew index RAM storage is allocated;\nnew index attribute and dictionary data is asynchronously preloaded to RAM;\non success, old index is deallocated and both indexes' files are renamed;\non failure, new index is deallocated;\nat any given moment, queries are served either from old or new index copy.\n\n\n\nSeamless rotate comes at the cost of higher peak\nmemory usage during the rotation (because both old and new copies of\n.spa/.spi/.spm data need to be in RAM while\npreloading new copy). Average usage stays the same.\nExample:\nseamless_rotate = 1"
    pid_file:
        link: 'http://sphinxsearch.com/docs/current.html#conf-pid-file'
        multi_value: false
        description: "searchd process ID file name.\nMandatory.\n\nPID file will be re-created (and locked) on startup. It will contain\nhead daemon process ID while the daemon is running, and it will be unlinked\non daemon shutdown. It's mandatory because Sphinx uses it internally\nfor a number of things: to check whether there already is a running instance\nof searchd; to stop searchd;\nto notify it that it should rotate the indexes. Can also be used for\ndifferent external automation scripts.\nExample:\npid_file = /var/run/searchd.pid"
    max_children:
        link: 'http://sphinxsearch.com/docs/current.html#conf-max-children'
        multi_value: false
        description: "Maximum amount of children to fork (or in other words, concurrent searches to run in parallel).\nOptional, default is 0 (unlimited).\n\nUseful to control server load. There will be no more than this much concurrent\nsearches running, at all times. When the limit is reached, additional incoming\nclients are dismissed with temporarily failure (SEARCHD_RETRY) status code\nand a message stating that the server is maxed out.\nExample:\nmax_children = 10"
    client_timeout:
        link: 'http://sphinxsearch.com/docs/current.html#conf-client-timeout'
        multi_value: false
        description: "Maximum time to wait between requests (in seconds) when using\npersistent connections. Optional, default is five minutes.\nExample:\nclient_timeout = 3600"
    read_timeout:
        link: 'http://sphinxsearch.com/docs/current.html#conf-read-timeout'
        multi_value: false
        description: "Network client request read timeout, in seconds.\nOptional, default is 5 seconds.\nsearchd will forcibly close the client connections which fail to send a query within this timeout.\nExample:\nread_timeout = 1"
    query_log_format:
        link: 'http://sphinxsearch.com/docs/current.html#conf-query-log-format'
        multi_value: false
        description: "Query log format.\nOptional, allowed values are 'plain' and 'sphinxql', default is 'plain'.\nIntroduced in version 2.0.1-beta.\n\nStarting with version 2.0.1-beta, two different log formats are supported.\nThe default one logs queries in a custom text format. The new one logs\nvalid SphinxQL statements. This directive allows to switch between the two\nformats on search daemon startup. The log format can also be altered\non the fly, using SET GLOBAL query_log_format=sphinxql syntax.\nRefer to Section\_5.9, “searchd query log formats” for more discussion and format\ndetails.\nExample:\nquery_log_format = sphinxql"
    query_log:
        link: 'http://sphinxsearch.com/docs/current.html#conf-query-log'
        multi_value: false
        description: "Query log file name.\nOptional, default is empty (do not log queries).\nAll search queries will be logged in this file. The format is described in Section\_5.9, “searchd query log formats”.\n\nIn case of 'plain' format, you can use the 'syslog' as the path to the log file.\nIn this case all search queries will be sent to syslog daemon with LOG_INFO priority,\nprefixed with '[query]' instead of timestamp.\nTo use the syslog option the sphinx must be configured '--with-syslog' on building.\nExample:\nquery_log = /var/log/query.log"
    log:
        link: 'http://sphinxsearch.com/docs/current.html#conf-log'
        multi_value: false
        description: "Log file name.\nOptional, default is 'searchd.log'.\nAll searchd run time events will be logged in this file.\n\nAlso you can use the 'syslog' as the file name. In this case the events will be sent to syslog daemon.\nTo use the syslog option the sphinx must be configured '--with-syslog' on building.\nExample:\nlog = /var/log/searchd.log"
    listen:
        link: 'http://sphinxsearch.com/docs/current.html#conf-listen'
        multi_value: true
        description: "This setting lets you specify IP address and port, or Unix-domain\nsocket path, that searchd will listen on.\nIntroduced in version 0.9.9-rc1.\n\nThe informal grammar for listen setting is:\n\nlisten = ( address \":\" port | port | path ) [ \":\" protocol ]\n\nI.e. you can specify either an IP address (or hostname) and port\nnumber, or just a port number, or Unix socket path. If you specify\nport number but not the address, searchd will listen on\nall network interfaces. Unix path is identified by a leading slash.\n\nStarting with version 0.9.9-rc2, you can also specify a protocol\nhandler (listener) to be used for connections on this socket.\nSupported protocol values are 'sphinx' (Sphinx 0.9.x API protocol)\nand 'mysql41' (MySQL protocol used since 4.1 upto at least 5.1).\nMore details on MySQL protocol support can be found in\nSection\_5.10, “MySQL protocol support and SphinxQL” section.\nExamples:\nlisten = localhost\nlisten = localhost:5000\nlisten = 192.168.0.1:5000\nlisten = /var/run/sphinx.s\nlisten = 9312\nlisten = localhost:9306:mysql41\n\nThere can be multiple listen directives, searchd will\nlisten for client connections on all specified ports and sockets.  If\nno listen directives are found then the server will listen\non all available interfaces using the default SphinxAPI port 9312.\nStarting with 1.10-beta, it will also listen on default SphinxQL\nport 9306. Both port numbers are assigned by IANA (see\nhttp://www.iana.org/assignments/port-numbers\nfor details) and should therefore be available.\n\nUnix-domain sockets are not supported on Windows."
common:
    plugin_dir:
        link: 'http://sphinxsearch.com/docs/current.html#conf-plugin-dir'
        multi_value: false
        description: "Trusted location for the dynamic libraries (UDFs).\nOptional, default is empty (no location).\nIntroduced in version 2.0.1-beta.\n\nSpecifies the trusted directory from which the\nUDF libraries can be loaded. Requires\nworkers = thread to take effect.\nExample:\nplugin_dir = /usr/local/sphinx/lib"
    rlp_max_batch_docs:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rlp-max-batch-docs'
        multi_value: false
        description: "Maximum number of documents batched before processing them by the RLP. Optional, default is 50.\nThis option has effect only if morphology = rlp_chinese_batched is specified.\nAdded in 2.2.1-beta.\nExample:\nrlp_max_batch_docs = 100"
    rlp_max_batch_size:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rlp-max-batch-size'
        multi_value: false
        description: "Maximum total size of documents batched before processing them by the RLP. Optional, default is 51200.\nDo not set this value to more than 10Mb because sphinx splits large documents to 10Mb chunks before processing them by the RLP.\nThis option has effect only if morphology = rlp_chinese_batched is specified.\nAdded in 2.2.1-beta.\nExample:\nrlp_max_batch_size = 100k"
    rlp_environment:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rlp-environment'
        multi_value: false
        description: "RLP environment configuration file. Mandatory if RLP is used.\nAdded in 2.2.1-beta.\nExample:\nrlp_environment = /home/myuser/RLP/rlp-environment.xml"
    rlp_root:
        link: 'http://sphinxsearch.com/docs/current.html#conf-rlp-root'
        multi_value: false
        description: "Path to the RLP root folder. Mandatory if RLP is used.\nAdded in 2.2.1-beta.\nExample:\nrlp_root = /home/myuser/RLP"
    json_autoconv_keynames:
        link: 'http://sphinxsearch.com/docs/current.html#conf-json-autoconv-keynames'
        multi_value: false
        description: "Whether and how to auto-convert key names within JSON attributes.\nKnown value is 'lowercase'.\nOptional, default value is unspecified (do not convert anything).\nAdded in 2.1.1-beta.\n\nWhen this directive is set to 'lowercase', key names within JSON attributes\nwill be automatically brought to lower case when indexing.\nThis conversion applies to any data source, that is, JSON attributes originating\nfrom either SQL or XMLpipe2 sources will all be affected.\nExample:\njson_autoconv_keynames = lowercase"
    json_autoconv_numbers:
        link: 'http://sphinxsearch.com/docs/current.html#conf-json-autoconv-numbers'
        multi_value: false
        description: "Automatically detect and convert possible JSON\nstrings that represent numbers, into numeric attributes.\nOptional, default value is 0 (do not convert strings into numbers).\nAdded in 2.1.1-beta.\n\nWhen this option is 1, values such as \"1234\" will be indexed as numbers instead\nof strings; if the option is 0, such values will be indexed as strings.\nThis conversion applies to any data source, that is, JSON attributes originating\nfrom either SQL or XMLpipe2 sources will all be affected.\nExample:\njson_autoconv_numbers = 1"
    on_json_attr_error:
        link: 'http://sphinxsearch.com/docs/current.html#conf-on-json-attr-error'
        multi_value: false
        description: "What to do if JSON format errors are found.\nOptional, default value is ignore_attr (ignore errors).\nApplies only to sql_attr_json attributes.\nAdded in 2.1.1-beta.\n\nBy default, JSON format errors are ignored (ignore_attr) and\nthe indexer tool will just show a warning. Setting this option to fail_index\nwill rather make indexing fail at the first JSON format error.\nExample:\non_json_attr_error = ignore_attr"
    lemmatizer_base:
        link: 'http://sphinxsearch.com/docs/current.html#conf-lemmatizer-base'
        multi_value: false
        description: "Lemmatizer dictionaries base path.\nOptional, default is /usr/local/share (as in --datadir switch to ./configure script).\nAdded in version 2.1.1-beta.\n\nOur lemmatizer implementation (see Section\_12.2.6, “morphology” for a discussion\nof what lemmatizers are) is dictionary driven. lemmatizer_base directive configures\nthe base dictionary path. File names are hardcoded and specific to a given lemmatizer;\nthe Russian lemmatizer uses ru.pak dictionary file. The dictionaries can be obtained\nfrom the Sphinx website.\nExample:\nlemmatizer_base = /usr/local/share/sphinx/dicts/"
